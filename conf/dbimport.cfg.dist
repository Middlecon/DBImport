[Database]
mysql_hostname = localhost
mysql_port = 3306
mysql_database = DBImport
mysql_username = dbimport
mysql_password = dbimport
aws_secret_name = 

[AWS]
# General AWS configuration that will be used by all parts of DBImport. Individual settings are located under the relevant section
region = 

[Kerberos]
# Used internally by DBImport to generate a fresh ticket at the start of every execution
keytab = keytab.file
principal = 

[Metastore]
# From version 0.81 of DBImport, it now supports multiple different versions of Hive Metastore implementations.
# The default where DBImport connects directly to the underlaying SQL database is still the default.
# Valid options are:
# hive_direct - Connects directly to the Hive SQL database using SqlAlchemy. hive_metastore_alchemy_conn is required
# glue_catalog - Using the AWS Hive Metastore implementation in Glue
metastore_type = hive_direct

# The SqlAlchemy connection string to the Hive metadata database.
# SqlAlchemy supports many different database engine, more information
# their website. This is required if metastore_type = hive_direct
hive_metastore_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/hive_metastore

[Hive]
# Connection details to Hive. For Hive 2.x, this needs to be a LLAP server
# The server must be running binary as the transport method. Http is not supported
servers = hiveserver1.localdomain:10000,hiveserver2.localdomain:10000
kerberos_service_name = hive
kerberos_realm = REALM
use_ssl = false

[Impala]
driver = /path/to/ImpalaJDBC41.jar
class = com.cloudera.impala.jdbc.DataSource
address = jdbc:impala://impalaserver1.localdomain:21050;AuthMech=1;SSL=1;KrbHostFQDN=xxxxx;KrbServiceName=impala

[Spark]
spark_major_version = 3

# Spark 2 CDP - Change versions to current version of CDP
# path_append = /opt/cloudera/parcels/CDH/lib/spark/python, /opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip, /opt/cloudera/parcels/CDH/lib/hive_warehouse_connector/pyspark_hwc-1.0.0.7.1.8.27-5.zip
# jar_files = /opt/cloudera/parcels/CDH/jars/hive-warehouse-connector-assembly-1.0.0.7.1.8.27-5.jar
# py_files = /opt/cloudera/parcels/CDH/lib/hive_warehouse_connector/pyspark_hwc-1.0.0.7.1.8.27-5.zip

# Spark 3 CDP - Change versions to current version of CDP
path_append = /opt/cloudera/parcels/SPARK3/lib/spark3/python, /opt/cloudera/parcels/SPARK3/lib/spark3/python/lib/py4j-0.10.9.5-src.zip, /opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/pyspark_hwc-spark3-1.0.0.3.3.7180.5-1.zip
jar_files = /opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/hive-warehouse-connector-spark3-assembly-1.0.0.3.3.7180.5-1.jar
py_files = /opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/pyspark_hwc-spark3-1.0.0.3.3.7180.5-1.zip
packages = 

# Spark 3 - AWS
# path_append = /usr/lib/spark/python, /usr/lib/spark/python/lib/py4j-src.zip
# jar_files =
# packages = org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3,org.apache.iceberg:iceberg-aws-bundle:1.4.3
# py_files =

# Yarn config for Spark
master = yarn
deployMode = client
yarnqueue = default

# Executor configuration
dynamic_allocation = true
executor_memory = 2688M

# For HDP 2.x, use HiveContext. For HDP 3.x and CDP, use HiveWarehouseSession.
hive_library = HiveWarehouseSession

[Sqoop]
yarnqueue = default

[Airflow]
# The mode Airflow is running in. Supported options are 'default' and 'aws_mwaa'. This impacts how commands are started
airflow_mode = default

# The SqlAlchemy connection string to the Airflow database.
# SqlAlchemy supports many different database engine, more information
# their website. This is only needed if pool is created with a database connection. Pool creation with Task does not need this
airflow_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/airflow

[Atlas]
# In order to have lineage in Atlas for DBImport, you need to specify the webaddress to Atlas. 
# If you dont want Atlas integration, just specify an empty address.
address = https://localhost:21443
timeout = 5
ssl_verify = true
ssl_cert_path = /etc/ssl/certs/ca-bundle.crt
# Authentication_type have two valid settings. 'username' or 'kerberos'
authentication_type = kerberos
username = 
password = 

[Credentials]
# You need a private/public key in able to encrypt and decrypt the username and password for the jdbc connections
# To generate such a pair, please use the following command and then point out the path to the keys
# If the path dont start with a /, the DBIMPORT_HOME will be used as a starting point
# openssl genrsa -out dbimport_private_key.pem 8192
# openssl rsa -in dbimport_private_key.pem -out dbimport_public_key.pem -outform PEM -pubout
# If you are using AWS Secrets Manager for passwords, then this is not needed
private_key = conf/dbimport_private_key.pem 
public_key = conf/dbimport_public_key.pem 

[Server]
# Setting to control the server daemon part of DBImport.
logdir = /var/log/dbimport
pidfile = /var/run/dbimport/dbimport_server.pid
distCP_threads = 20
distCP_separate_logs = true
distCP_yarnqueue = default
atlas_threads = 8
restServer_address = 0.0.0.0
restServer_port = 5188
restServer_workers = 4
restServer_keyfile = host.key
restServer_certfile = host.cert

[Anonymization]
# In order to get the same hash value every time you need to specify a seed. The seed can be any alpanumeric string. You can also override
# the seed per connection with the help of a values from a string. See more in table jdbc_connections for this setting.
# The seed is limited to 16 chars
seed = Change_Me!

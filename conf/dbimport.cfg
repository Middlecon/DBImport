[Database]
mysql_hostname = localhost
mysql_port = 3306
mysql_database = DBImport
mysql_username = dbimport
mysql_password = dbimport

[Kerberos]
# Used internally by DBImport to generate a fresh ticket at the start of every execution
keytab=keytab.file
principal=user@REALM

[Metastore]
# From version 0.81 of DBImport, it now supports multiple different versions of Hive Metastore implementations.
# The default where DBImport connects directly to the underlaying SQL database is still the default.
# Valid options are:
# hive_direct - Connects directly to the Hive SQL database using SqlAlchemy. hive_metastore_alchemy_conn is required
# glue_catalog - Using the AWS Hive Metastore implementation in Glue
metastore_type = hive_direct

# The SqlAlchemy connection string to the Hive metadata database.
# SqlAlchemy supports many different database engine, more information
# their website. This is required if metastore_type = hive_direct
hive_metastore_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/hive_metastore

[Hive]
# Connection details to Hive. For Hive 2.x, this needs to be a LLAP server
# The server must be running binary as the transport method. Http is not supported
servers = hiveserver1.localdomain:10000,hiveserver2.localdomain:10000
kerberos_service_name = hive
kerberos_realm = REALM

# Specify if the Hive server is running SSL or not
use_ssl = false

[Impala]
driver = /path/to/ImpalaJDBC41.jar
class = com.cloudera.impala.jdbc.DataSource
address = jdbc:impala://impalaserver1.localdomain:21050;AuthMech=1;SSL=1;KrbHostFQDN=xxxxx;KrbServiceName=impala

[Spark]
# Spark 2 - Change path to current version of CDP
# path_append = /opt/cloudera/parcels/CDH/lib/spark/python, /opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip, /opt/cloudera/parcels/CDH/lib/hive_warehouse_connector/pyspark_hwc-1.0.0.7.1.8.27-5.zip
# jar_files = /opt/cloudera/parcels/CDH/jars/hive-warehouse-connector-assembly-1.0.0.7.1.8.27-5.jar
# py_files = /opt/cloudera/parcels/CDH/lib/hive_warehouse_connector/pyspark_hwc-1.0.0.7.1.8.27-5.zip

# Spark 3 - Change path to current version of CDP
path_append = /opt/cloudera/parcels/SPARK3/lib/spark3/python, /opt/cloudera/parcels/SPARK3/lib/spark3/python/lib/py4j-0.10.9.5-src.zip, /opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/pyspark_hwc-spark3-1.0.0.3.3.7180.5-1.zip
jar_files = /opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/hive-warehouse-connector-spark3-assembly-1.0.0.3.3.7180.5-1.jar
# jar_files = /opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/hive-warehouse-connector-spark3-assembly-1.0.0.3.3.7180.5-1.jar,/path/to/iceberg-spark-runtime-3.3_2.12-1.1.0.jar
py_files = /opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/pyspark_hwc-spark3-1.0.0.3.3.7180.5-1.zip


# Yarn config for Spark
master = yarn
deployMode = client
yarnqueue = default

# Executor configuration
dynamic_allocation = true
executor_memory = 2688M

# For HDP 2.x, use HiveContext. For HDP 3.x and CDP, use HiveWarehouseSession.
hive_library = HiveWarehouseSession

[Sqoop]
yarnqueue = default

[Airflow]
# The SqlAlchemy connection string to the Airflow database.
# SqlAlchemy supports many different database engine, more information
# their website
airflow_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/airflow

[Atlas]
# In order to have lineage in Atlas for DBImport, you need to specify the webaddress to Atlas. 
# If you dont want Atlas integration, just specify an empty address.
address = https://localhost:21443
timeout = 5
ssl_verify = true
ssl_cert_path = /etc/ssl/certs/ca-bundle.crt
# Authentication_type have two valid settings. 'username' or 'kerberos'
authentication_type = kerberos
username = 
password = 

[Credentials]
# You need a private/public key in able to encrypt and decrypt the username and password for the jdbc connections
# To generate such a pair, please use the following command and then point out the path to the keys
# If the path dont start with a /, the DBIMPORT_HOME will be used as a starting point
# openssl genrsa -out dbimport_private_key.pem 8192
# openssl rsa -in dbimport_private_key.pem -out dbimport_public_key.pem -outform PEM -pubout

private_key = conf/dbimport_private_key.pem 
public_key = conf/dbimport_public_key.pem 

[Server]
# Setting to control the server daemon part of DBImport.
logdir = /var/log/dbimport
pidfile = /var/run/dbimport/dbimport_server.pid
distCP_threads = 20
distCP_separate_logs = true
distCP_yarnqueue = default
atlas_threads = 8
restServer_address = 0.0.0.0
restServer_port = 5188
restServer_workers = 4
restServer_keyfile = host.key
restServer_certfile = host.cert

[Anonymization]
# In order to get the same hash value every time you need to specify a seed. The seed can be any alpanumeric string. You can also override
# the seed per connection with the help of a values from a string. See more in table jdbc_connections for this setting.
# The seed is limited to 16 chars
seed = Change_Me!

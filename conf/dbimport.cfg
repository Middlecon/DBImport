[Database]
mysql_hostname = localhost
mysql_port = 3306
mysql_database = DBImport
mysql_username = dbimport
mysql_password = dbimport

[REST_statistics]
# Should the configuration data about the imported columns from the source systems be posted to the REST interface?
post_column_data = false

# Should the import statistics be posted to the REST interface?
post_import_data = false

# HTTP Timeout
timeout = 5

#URL to REST interface
rest_endpoint = "https://localhost:8080/rest"

[Kerberos]
# Used internally by DBImport to generate a fresh ticket at the start of every execution
keytab=keytab.file
principal=user@REALM

[Hive]
# The SqlAlchemy connection string to the Hive metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
hive_metastore_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/hive_metastore

# Connection details to Hive. For Hive 2.x, this needs to be a LLAP server
# The server must be running binary as the transport method. Http is not supported
servers = hiveserver1.localdomain:10000,hiveserver2.localdomain:10000
kerberos_service_name = hive
kerberos_realm = REALM

# Specify if the Hive server is running SSL or not
use_ssl = false

# Minimum an Maximum number of buckets for ACID tables during Merge
min_buckets = 1
max_buckets = 1024

[Spark]
# HDP 3.x
path_append = /usr/hdp/current/spark2-client/python/, /usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip, /usr/hdp/current/hive_warehouse_connector/pyspark_hwc-1.0.0.3.1.4.0-315.zip
jar_files = /usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.4.0-315.jar
py_files = /usr/hdp/current/hive_warehouse_connector/pyspark_hwc-1.0.0.3.1.4.0-315.zip

# HDP 2.x
# path_append = /usr/hdp/current/spark2-client/python/, /usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip
# jar_files =
# py_files = /usr/hdp/current/spark2-client/python/lib/py4j-src.zip

# Yarn config for Spark
master = yarn
deployMode = client
yarnqueue = default

# Executor configuration
dynamic_allocation = true
min_executors = 0
max_executors = 20
executor_memory = 2688M

# Important that you specify this to the HDP version you are running. 
hdp_version = 3.1.4.0-315

# For HDP 2.x, use HiveContext. For HDP 3.x, use HiveWarehouseSession.
hive_library = HiveWarehouseSession

[Sqoop]
yarnqueue = default

[Airflow]
# The SqlAlchemy connection string to the Airflow database.
# SqlAlchemy supports many different database engine, more information
# their website
airflow_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/airflow

[Atlas]
# In order to have lineage in Atlas for DBImport, you need to specify the webaddress to Atlas. 
# If you dont want Atlas integration, just specify an empty address.
address = https://localhost:21443
timeout = 5
ssl_verify = true
ssl_cert_path = /etc/ssl/certs/ca-bundle.crt

[Credentials]
# You need a private/public key in able to encrypt and decrypt the username and password for the jdbc connections
# To generate such a pair, please use the following command and then point out the path to the keys
# If the path dont start with a /, the DBIMPORT_HOME will be used as a starting point
# openssl genrsa -out dbimport_private_key.pem 8192
# openssl rsa -in dbimport_private_key.pem -out dbimport_public_key.pem -outform PEM -pubout

private_key = conf/dbimport_private_key.pem 
public_key = conf/dbimport_public_key.pem 

[Server]
# Setting to control the server daemon part of DBImport.
logdir = /var/log/dbimport
pidfile = /var/run/dbimport/dbimport_server.pid
distCP_threads = 20
distCP_separate_logs = true
distCP_yarnqueue = default
restServer_address = 0.0.0.0
restServer_port = 5188


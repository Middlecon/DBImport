{
  "table": {
    "import": {
      "database": "Name of Hive Database to import to",
      "table": "Name of Hive Table to import to",
      "connection": "Name of database connection from jdbc_connections table",
      "sourceSchema": "Name of the schema in the remote database",
      "sourceTable": "Name of the table in the remote database",
      "importPhaseType": "What import method to use",
      "etlPhaseType": "What method to use for ETL phase",
      "importTool": "What tool should be used for importing data",
      "etlEngine": "What engine will be used to process etl stage",
      "lastUpdateFromSource": "Timestamp of last schema update from source",
      "sourceTableType": "Type of table on the source system. This is a read-only",
      "importDatabase": "Override the database name used for the import table",
      "importTable": "Override the table name used for the import table",
      "historyDatabase": "Override the database name used for the history table if that exists",
      "historyTable": "Override the table name used for the history table if that exists",
      "airflowPriority": "This will set priority_weight in Airflow",
      "includeInAirflow": "Will the table be included in Airflow DAG when it matches the DAG selection",
      "operatorNotes": "Free text field to write a note about the import.",
      "validateImport": "Should the import be validated",
      "validationMethod": "Validation method to use",
      "validateSource": "query = Run a 'select count(*) from ...' to get the number of rows in the source table. sqoop = Use the number of rows imported by sqoop as the number of rows in the source table",
      "validateDiffAllowed": "-1 = auto calculated diff allowed. If a positiv number, this is the amount of rows that the diff is allowed to have",
      "sourceRowcount": "Used for validation. Dont change manually",
      "sourceRowcountIncr": "",
      "targetRowcount": "Used for validation. Dont change manually",
      "validationCustomQuerySourceSQL": "Custom SQL query for source table",
      "validationCustomQueryHiveSQL": "Custom SQL query for Hive table",
      "validationCustomQueryValidateImportTable": "true = Validate Import table, false = Dont validate Import table",
      "validationCustomQuerySourceValue": "Used for validation. Dont change manually",
      "validationCustomQueryHiveValue": "Used for validation. Dont change manually",
      "truncateTable": "",
      "allowTextSplitter": "Allow splits on text columns. Use with caution",
      "forceString": "If set to 1, all character based fields (char, varchar) will become string in Hive. Overrides the same setting in jdbc_connections table",
      "splitByColumn": "Column to split by when doing import with multiple sessions",
      "sqlWhereAddition": "Will be added AFTER the SQL WHERE. If it's an incr import, this will be after the incr limit statements. Example: orderId > 1000",
      "customQuery": "Use a custom query in sqoop to read data from source table",
      "customMaxQuery": "You can use a custom SQL query that will get the Max value from the source database. This Max value will be used in an inremental import to know how much to read in each execution",
      "useGeneratedSql": "true = Use the generated SQL that is saved in the generated_sqoop_query column",
      "nomergeIngestionSqlAddition": "This will be added to the data ingestion of None-Merge imports (full, full_direct and incr). Usefull to filter out data from import tables to target tables",
      "sqoopOptions": "Options to send to sqoop. Most common used for --split-by option",
      "lastSize": "Used to track sqoop operation. Dont change manually",
      "lastRows": "Used to track sqoop operation. Dont change manually",
      "lastMappers": "Used to track sqoop operation. Dont change manually",
      "generatedHiveColumnDefinition": "Generated column definition for Hive create table. Dont change manually",
      "generatedSqoopQuery": "Generated query for sqoop. Dont change manually",
      "generatedSqoopOptions": "Generated options for sqoop. Dont change manually",
      "generatedPkColumns": "Generated Primary Keys. Dont change manually",
      "generatedForeignKeys": "",
      "incrMode": "Incremental import mode",
      "incrColumn": "What column to use to identify new rows",
      "incrValidationMethod": "full or incr. Full means that the validation will check to total number of rows up until maxvalue and compare source with target. Incr will only compare the rows between min and max value (the data that sqoop just wrote)",
      "incrMinvalue": "Used for incremental imports. Dont change manually",
      "incrMaxvalue": "Used for incremental imports. Dont change manually",
      "incrMinvaluePending": "Used for incremental imports. Dont change manually",
      "incrMaxvaluePending": "Used for incremental imports. Dont change manually",
      "createForeignKeys": "-1 (default) = Get information from jdbc_connections table",
      "invalidateImpala": "-1 = Use default value from configuration table, otherwise 1 will invalidate the table in Impala and 0 will not",
      "softDeleteDuringMerge": "If 1, then the row will be marked as deleted instead of actually being removed from the table. Only used for Merge imports",
      "pkColumnOverride": "Force the import and Hive table to define another PrimaryKey constraint. Comma separeted list of columns",
      "pkColumnOverrideMergeonly": "Force the import to use another PrimaryKey constraint during Merge operations. Comma separeted list of columns",
      "mergeCompactionMethod": "Compaction method to use after import using merge is completed. Default means a major compaction if it is configured to do so in the configuration table",
      "datalakeSource": "This value will come in the dbimport_source column if present. Overrides the same setting in jdbc_connections table",
      "mappers": "-1 = auto or positiv number for a fixed number of mappers. If Auto, then it's calculated based of last sqoop import size",
      "splitCount": "Sets tez.grouping.split-count in the Hive session",
      "mergeHeap": "Should be a multiple of Yarn container size. If NULL then it will use the default specified in Yarn and TEZ",
      "sparkExecutorMemory": "Memory used by spark when importring data. Overrides default value in global configuration",
      "sparkExecutors": "Number of Spark executors to use. Overrides default value in global configuration",
      "copyFinished": "Time when last copy from Master DBImport instance was completed. Dont change manually",
      "copySlave": "Defines if this table is a Master table or a Slave table. Dont change manually",
      "columns": {
        "columnName": "Name of column in Hive. Dont change this manually",
        "columnOrder": "In what order does the column exist in the source system",
        "sourceColumnName": "Name of column in source system. Dont change this manually",
        "columnType": "Column type in Hive. Dont change this manually",
        "sourceColumnType": "Column type in source system. Dont change this manually",
        "sourceDatabaseType": "That database type was the column imported from",
        "columnNameOverride": "Set a custom name of the column in Hive",
        "columnTypeOverride": "Set a custom column type in Hive",
        "sqoopColumnType": "Used to create a correct --map-column-java setting for sqoop.",
        "sqoopColumnTypeOverride": "Set the --map-column-java field to a fixed value and not calculated by DBImport",
        "forceString": "If set to 1, all character based fields (char, varchar) will become string in Hive. Overrides the same setting in import_tables and jdbc_connections table",
        "includeInImport": "true = Include column in import, false = Exclude column in import",
        "sourcePrimaryKey": "Number starting from 1 listing the order of the column in the PK. Dont change this manually",
        "lastUpdateFromSource": "Timestamp of last schema update from source",
        "comment": "The column comment from the source system",
        "operatorNotes": "Free text field to write a note about the column",
        "anonymizationFunction": "What anonymization function should be used with the data in this column"
      }
    },
    "export": {
      "connection": "Database connection name that we export to",
      "targetSchema": "Schema on the target system",
      "targetTable": "Table on the target system",
      "exportType": "What export method to use. Only full and incr is supported.",
      "exportTool": "What tool should be used for exporting data",
      "database": "Name of Hive Database that we export from",
      "table": "Name of Hive Table that we export from",
      "lastUpdateFromHive": "Timestamp of last schema update from Hive",
      "sqlWhereAddition": "Will be added AFTER the SQL WHERE. If it's an incr export, this will be after the incr limit statements. Example: orderId > 1000",
      "includeInAirflow": "Will the table be included in Airflow DAG when it matches the DAG selection",
      "airflowPriority": "This will set priority_weight in Airflow",
      "forceCreateTempTable": "Force export to create a Hive table and export that instead. Useful when exporting views",
      "validateExport": "true = Validate the export once it's done. false = Disable validation",
      "validationMethod": "Validation method to use",
      "validationCustomQueryHiveSQL": "Custom SQL query for Hive table",
      "validationCustomQueryTargetSQL": "Custom SQL query for target table",
      "uppercaseColumns": "-1 = auto (Oracle = uppercase, other databases = lowercase)",
      "truncateTarget": "true = Truncate the target table before we export the data. Not used by incremental exports",
      "mappers": "-1 = auto, 0 = invalid. Auto updated by 'export_main.sh'",
      "tableRowcount": "Number of rows in Hive table. Dont change manually",
      "targetRowcount": "Number of rows in Target table. Dont change manually",
      "validationCustomQueryHiveValue": "Used for validation. Dont change manually",
      "validationCustomQueryTargetValue": "Used for validation. Dont change manually",
      "incrColumn": "The column in the Hive table that will be used to identify new rows for the incremental export. Must be a timestamp column",
      "incrValidationMethod": "full or incr. Full means that the validation will check to total number of rows up until maxvalue and compare source with target. Incr will only compare the rows between min and max value (the data that sqoop just wrote)",
      "incrMinvalue": "Used by incremental exports to keep track of progress. Dont change manually",
      "incrMaxvalue": "Used by incremental exports to keep track of progress. Dont change manually",
      "incrMinvaluePending": "Used by incremental exports to keep track of progress. Dont change manually",
      "incrMaxvaluePending": "Used by incremental exports to keep track of progress. Dont change manually",
      "sqoopOptions": "Sqoop options to use during export.",
      "lastSize": "Used to track sqoop operation. Dont change manually",
      "lastRows": "Used to track sqoop operation. Dont change manually",
      "lastMappers": "Used to track sqoop operation. Dont change manually",
      "lastExecution": "Used to track sqoop operation. Dont change manually",
      "javaHeap": "Heap size for Hive",
      "createTargetTableSql": "SQL statement that was used to create the target table. Dont change manually",
      "operatorNotes": "Free text field to write a note about the export.",
      "columns": {
        "columnName": "Name of column in target table. Dont change this manually",
        "columnType": "Column type from Hive. Dont change this manually",
        "columnOrder": "The order of the columns. Dont change this manually",
        "targetColumnName": "Override the name of column in the target system",
        "targetColumnType": "Override the column type in the target system",
        "lastUpdateFromHive": "Timestamp of last schema update from Hive. Dont change this manually",
        "includeInExport": "1 = Include column in export, 0 = Exclude column in export",
        "comment": "The column comment from the source system. Dont change this manually",
        "operatorNotes": "Free text field to write a note about the import."
      }
    }
  },
  "airflow": {
    "import": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute dag",
      "filterTable": "Filter string for database and table. ; separated. Wildcards (*) allowed. Example HIVE_DB.HIVE_TABLE; HIVE_DB.HIVE_TABLE",
      "finishAllStage1First": "true = All Import phase jobs will be completed first, and when all is successfull, the ETL phase start",
      "runImportAndEtlSeparate": "true = The Import and ETL phase will run in separate Tasks",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "retriesStage1": "Specific retries number for Import Phase",
      "retriesStage2": "Specific retries number for ETL Phase",
      "poolStage1": "Airflow pool used for stage1 tasks. NULL for the default Hostname pool",
      "poolStage2": "Airflow pool used for stage2 tasks. NULL for the default DAG pool",
      "operatorNotes": "Free text field to write a note about the import DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "autoRegenerateDag": "true = The DAG will be auto regenerated by manage command",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "metadataImport": "true = Run only getSchema and getRowCount, false = Run a normal import",
      "timezone": "Timezone used for schedule_interval column. Use full text timezone, example Europe/Stockholm",
      "email": "Email to send message to in case email_on_retry or email_on_failure is set to True",
      "emailOnFailure": "Send email on failures",
      "emailOnRetries": "Send email on retries",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "true = Use the retry_exponential_backoff Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, \nfalse = Run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "export": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute dag",
      "filterConnection": "Filter string for DBALIAS in export_tables",
      "filterTargetSchema": "Filter string for TARGET_SCHEMA  in export_tables",
      "filterTargetTable": "Filter string for TARGET_TABLE  in export_tables",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "operatorNotes": "Free text field to write a note about the export DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "autoRegenerateDag": "true = The DAG will be auto regenerated by manage command",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "timezone": "Timezone used for schedule_interval column. Use full text timezone, example Europe/Stockholm",
      "email": "Email to send message to in case email_on_retry or email_on_failure is set to True",
      "emailOnFailure": "Send email on failures",
      "emailOnRetries": "Send email on retries",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "true = Use the retry_exponential_backoff Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, false = Run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "custom": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute dag",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "operatorNotes": "Free text field to write a note about the custom DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "autoRegenerateDag": "true = The DAG will be auto regenerated by manage command",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "timezone": "Timezone used for schedule_interval column. Use full text timezone, example Europe/Stockholm",
      "email": "Email to send message to in case email_on_retry or email_on_failure is set to True",
      "emailOnFailure": "Send email on failures",
      "emailOnRetries": "Send email on retries",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "true = Use the retry_exponential_backoff Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, false = Run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "tasks": {
      "name": "Name of the Task in Airflow",
      "type": "The type of the Task",
      "placement": "Placement for the Task",
      "connection": "For  'JDBC SQL' Task Type, this specifies what database the SQL should run against",
      "airflowPool": "Airflow Pool to use",
      "airflowPriority": "Airflow Priority. Higher number, higher priority",
      "includeInAirflow": "Enable or disable the Task in the DAG during creation of DAG file",
      "taskDependencyDownstream": "Defines the upstream dependency for the Task. Comma separated list",
      "taskDependencyUpstream": "Defines the upstream dependency for the Task. Comma separated list",
      "taskConfig": "The configuration for the Task. Depends on what Task type it is",
      "sensorPokeInterval": "Poke interval for sensors in seconds",
      "sensorTimeoutMinutes": "Timeout for sensors in minutes",
      "sensorConnection": "Name of Connection in Airflow",
      "sensorSoftFail": "Setting this to 1 will add soft_fail=True on sensor",
      "sudoUser": "The task will use this user for sudo instead of default"
    }
  },
  "connection": {
    "name": "Name of the Database connection",
    "connectionString": "The JDBC URL String",
    "privateKeyPath": "",
    "publicKeyPath": "",
    "credentials": "Encrypted fields for credentials.m Changed by the saveCredentialTool",
    "source": "This value will come in the dbimport_source column if present. \nPriority is table, connection",
    "forceString": "If set to 1, all character based fields (char, varchar) will become string in Hive",
    "maxSessions": "You can limit the number of parallel sessions during import with this value. If NULL, then Max will come from configuration file",
    "createDatalakeImport": "If set to 1, the datalake_import column will be created on all tables that is using this dbalias",
    "timeWindowStart": "Start of the time window when we are allowed to run against this connection",
    "timeWindowStop": "End of the time window when we are allowed to run against this connection",
    "timeWindowTimezone": "Timezone used for timewindow_start and timewindow_stop columns. Use full text timezone, example Europe/Stockholm",
    "operatorNotes": "Free text field to write a note about the connection",
    "contactInformation": "Contact information. Used by Atlas integration",
    "description": "Description. Used by Atlas integration",
    "owner": "Owner of system and/or data. Used by Atlas integration",
    "environment": "Name of the Environment type",
    "seedFile": "File that will be used to fetch the custom seed that will be used for anonymization functions on data from the connection",
    "createForeignKey": "true = Create foreign keys, false = Dont create foreign keys",
    "atlasDiscovery": "true = Discover tables/views on this connection, false = Dont use Atlas discovery on this connection",
    "atlasIncludeFilter": "Include filter for Atlas discovery",
    "atlasExcludeFilter": "Exclude filter for Atlas discovery",
    "atlasLastDiscovery": ""
  }
}

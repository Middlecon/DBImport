{
  "table": {
    "import": {
      "database": "Name of the Database to import to",
      "table": "Name of the Table to import to",
      "connection": "Name of database connection. These are specified under 'Connection'",
      "sourceSchema": "Name of the schema in the remote database",
      "sourceTable": "Name of the table in the remote database",
      "importPhaseType": "What import method to use",
      "etlPhaseType": "What method to use for ETL phase",
      "importTool": "What tool should be used for importing data",
      "etlEngine": "What engine will be used to process etl stage",
      "lastUpdateFromSource": "Timestamp of last schema update from source. Fetching the schema is the first thing that happens during an import. During this part of the import, all columns are fetched and the list of columns will be updated. Before that, no columns exists for the table",
      "sourceTableType": "Information only\nType of table on the source system. Will show if this is a real table or a view",
      "importDatabase": "Override the database name used for the import table. Rules for default databases name is defined in 'Configurations'",
      "importTable": "Override the table name used for the import table. Rules for default table name is defined in 'Configurations'",
      "historyDatabase": "Override the database name used for the history table if that exists. Rules for default history databases name is defined in 'Configurations'",
      "historyTable": "Override the table name used for the history table if that exists. Rules for default history table name is defined in 'Configurations'",
      "airflowPriority": "This will set priority_weight in Airflow",
      "includeInAirflow": "Will the table be included in Airflow DAG when it matches the DAG selection",
      "operatorNotes": "Free text field to write a note about the import.",
      "validateImport": "Should the import be validated",
      "validationMethod": "Validation method to use\n\n'Row Count' - Will use the default method of counting the number of rows in both the source and target systems and compare the number. A certain diff can be allowed and this is defined in 'Allowed Validation Difference'\n\n'Custom Query' - Using this method will need two custom queries. One runs on the source system and one runs on the target system. Both queries must return one row with one column. The values in that single cell will be compared between the two queries. If they are the same, the validation is passed. You specify these custom queries in the 'Custom Query .... SQL' fields.",
      "validateSource": "There are two optins for how to validate the source database.\n\n'Query before import' - This will run a 'select count(*) from ...' to get the number of rows in the source tablei before the actual import starts.\n\n. 'Imported rows' = Use the number of rows imported by the import tool as the number of rows in the source table\n\nUsing the imported rows function can be used to get a valid row amount for tables that is in constant change and will create a to big of a diff between the time for the query and the time for the import tool to execute.",
      "validateDiffAllowed": "How many rows that is allowed to diff. Setting this to Auto will make DBImport to auto calculated ithe allowed diff. If no diff is allowed at all, set this to 0",
      "sourceRowcount": "Information only\nThis is the amount of rows that the validation identified in the source system on the last full import",
      "sourceRowcountIncr": "Informatation only\nThis is the amount of rows that the validation identified in the source system on the last incremental import",
      "targetRowcount": "Informatation only\nThe number of rows that exists in the target table after the operation is executed",
      "validationCustomQuerySourceSQL": "Query to be executed on the source table. Only used when 'Validation Method' is set to 'Custom Query'",
      "validationCustomQueryHiveSQL": "Query to be executed on the target table. Only used when 'Validation Method' is set to 'Custom Query'",
      "validationCustomQueryValidateImportTable": "",
      "validationCustomQuerySourceValue": "Information only\nLast value return from the custom query on the source table",
      "validationCustomQueryHiveValue": "Information only\nLast value return from the custom query on the target table",
      "allowTextSplitter": "When primary key or key specified to get uniqe value is a text column, like char or varchar, the default is not to allow to use that for splits. Setting this to True will allow the splits on text columns. ",
      "forceString": "If set to True, all character based fields (char, varchar) will become string in target database. \n\nIf Default is selected, then the value will come from the settings on the connection used for this import.",
      "splitByColumn": "Column to split by when doing import with multiple sessions",
      "sqlWhereAddition": "Add a custom where clause into the query used to fetch the data from the source table. This is added AFTER the SQL WHERE. If it's an incr import, this will be after the incr limit statements. \n\nExample: orderId > 1000\n\nA special value can also be used here. If the string ${MAX_VALUE} exists in this field, the value returned from the query specified in 'Custom Max Query' will replace the text ${MAX_VALUE} with that value. This is used during very complex imports where you cant read all data in the table due to the status of the data.",
      "customQuery": "Use a custom query to read data from source table",
      "customMaxQuery": "You can use a custom SQL query that will get the Max value from the source database. This Max value will be used in an inremental import to know how much to read in each execution. Can also replace the string ${MAX_VALUE} specified under 'SQL WHERE Addition'",
      "useGeneratedSql": "When an import is started, DBImport calculate and create a SQL statement that will be used to fetch all rows from the source table. Usually, this is fine and most import will use this generated SQL statement. But in some cases, you want to override the SQL statement and create your own. If you want to use your own, you specify this query in the 'Custom Query' field",
      "nomergeIngestionSqlAddition": "Will be added to the SQL WHERE statement that copy the data from the import to the target table. Will not affect the data imported from source table. Only active when a None-Merge imports is used (ETL Type not set to a Merge type). Usefull to filter out data",
      "sqoopOptions": "Custom Options to send to sqoop command.",
      "lastSize": "Information only\nThe size, in byte, of the amount of data that was transfered from source to import table in the last import",
      "lastRows": "Information only\nNumber of rows that was transfered from source to import table in the last import",
      "lastMappers": "Information only\nThe number of mappers used during the last import",
      "generatedHiveColumnDefinition": "Information only\nGenerated column definition for target table.",
      "generatedSqoopQuery": "Information only\nGenerated query for sqoop. This does not exist for Spark imports",
      "generatedSqoopOptions": "Information only\nGenerated options for sqoop command. This does not exist for Spark imports",
      "generatedPkColumns": "Information only\nGenerated Primary Keys.",
      "incrMode": "Incremental import mode\n\n'Last Modified - Will use a DATE or TIMESTAMP column to determine what rows to read incrementally\nAppend - Will use an integer based (INT, BIGINT..) column with increasing values to determine what rows to read incrementally",
      "incrColumn": "What column to use to identify new rows",
      "incrValidationMethod": "How should we validate the incremental import?\n\nFull - validation will check to total number of rows up until maxvalue and compare source with target.\nIncr - Only compare the rows that was imported",
      "incrMinvalue": "Information only\nThe min value used by the last import",
      "incrMaxvalue": "Information only\nThe max value used by the last import",
      "incrMinvaluePending": "Information only\nIf a value exists in this field, it means an incremental import is running or failed to run and this is the min value that was used",
      "incrMaxvaluePending": "Information only\nIf a value exists in this field, it means an incremental import is running or failed to run and this is the max value that was used",
      "createForeignKeys": "Should the import try to create the same Foreign Keys in the target databas as it exists on the source database? This can be specified here on the table or as a defalt value on the connection used. \nIf the foreign table does not exists in the target database, a warning text will be printed during import that the table is missing, but the import will still continue to import the data.",
      "invalidateImpala": "If Impala is used, you can set this to True and an 'invalidate metadata' query is executed in Impala at the end of the import to make sure the last imported data is visable. The default value is set under 'Configuration'",
      "softDeleteDuringMerge": "When set to True, the row will be marked as deleted instead of actually being removed from the table. Only used for Merge imports",
      "pkColumnOverride": "Force the import and target table to define another PrimaryKey constraint. Comma separeted list of columns",
      "pkColumnOverrideMergeonly": "Force the import to use another PrimaryKey constraint during Merge operations. Comma separeted list of columns",
      "mergeCompactionMethod": "Compaction method to use after import using merge methods is completed. Default means a major compaction if it is configured to do so in the configuration table",
      "datalakeSource": "This value will come in the dbimport_source column if present. Overrides the same setting in jdbc_connections table",
      "mappers": "Auto or positiv number for a fixed number of mappers. If Auto, then it's calculated based of last import size. \nIf the import is having problems to split the data, a common option is to set this to 1 to avoid splits all together",
      "splitCount": "Sets tez.grouping.split-count in the Hive session",
      "mergeHeap": "Sets hive.tez.container.size in the Hive session. This is an integer based value in MB. Should be a multiple of Yarn container size. If not set, then it will use the default specified in Yarn and TEZ\n\nExample: 3072 for 3G container size",
      "sparkExecutorMemory": "Memory used by spark when importring data. Overrides default value in global configuration",
      "sparkExecutors": "Number of Spark executors to use. Overrides default value in global configuration",
      "copyFinished": "Information only\nTime when last copy from Master DBImport instance was completed.",
      "copySlave": "Defines if this table is a Master table or a Slave table. If an import from the Master DBImport comes, this will be set to Slave automatically by DBImport. But in order to switch back, you need to manually set this to False.",
      "columns": {
        "columnName": "Information only\nName of column in target database This might not be the same as the column name in the source system due to special character handling",
        "columnOrder": "Information only\nOrder of the column",
        "sourceColumnName": "Information only\nName of the column in the source system. This might not be the same as the column name in the target system due to special character handling",
        "columnType": "Information only\nColumn type in target database. This might not be the same as the column type in source system due to support for different column types in different database systems.",
        "sourceColumnType": "Information only\nColumn type in srouce database. This might not be the same as the column type in target system due to support for different column types in different database systems.",
        "sourceDatabaseType": "Information only\nThe type of database",
        "columnNameOverride": "Set a custom name of the column",
        "columnTypeOverride": "Set a custom column type",
        "sqoopColumnType": "Used to create a correct --map-column-java setting for sqoop.",
        "sqoopColumnTypeOverride": "Set the --map-column-java field to a fixed value and not calculated by DBImport",
        "forceString": "If set to True, all character based fields (char, varchar) will become string in Hive. Overrides the same setting on the table or the connection",
        "includeInImport": "It's possible to include or exclude a column during import. That is controlled on this setting ",
        "sourcePrimaryKey": "Information only\nIf the column is part of the primary key, this indicates the order of the column in the primary key",
        "lastUpdateFromSource": "Timestamp of last schema update from source",
        "comment": "Information only\nThe column comment from the source system",
        "operatorNotes": "Free text field to write a note about the column",
        "anonymizationFunction": "What anonymization function should be used with the data in this column"
      }
    },
    "export": {
      "connection": "Database connection name that we export to",
      "targetSchema": "Schema on the target system",
      "targetTable": "Table on the target system",
      "exportType": "What export method to use.",
      "exportTool": "What tool should be used for exporting data.",
      "database": "Name of the database that we export from",
      "table": "Name of the table that we export from",
      "lastUpdateFromHive": "Timestamp of last schema update from source system. Fetching the schema is the first thing that happens during an export. During this part of the export, all columns are fetched and the list of columns on the export table will be updated. Before that, no columns exists for the table ",
      "sqlWhereAddition": "Add a custom where clause into the query used to fetch the data from the source table. This is added AFTER the SQL WHERE. If it's an incr export, this will be after the incr limit statements. \n\nExample: orderId > 1000",
      "includeInAirflow": "Will the table be included in Airflow DAG when it matches the DAG selection",
      "airflowPriority": "This will set priority_weight in Airflow",
      "forceCreateTempTable": "Force export to create a table and export that instead. Useful when exporting views",
      "validateExport": "Should the export be validated?",
      "validationMethod": "Validation method to use\n\n'Row Count' - Will use the default method of counting the number of rows in both the source and target systems and compare the number. \n\n'Custom Query' - Using this method will need two custom queries. One runs on the source system and one runs on the target system. Both queries must return one row with one column. The values in that single cell will be compared between the two queries. If they are the same, the validation is passed. You specify these custom queries in the 'Custom Query .... SQL' fields.",
      "validationCustomQueryHiveSQL": "Query to be executed on the source table. Only used when 'Validation Method' is set to 'Custom Query'",
      "validationCustomQueryTargetSQL": "Query to be executed on the target table. Only used when 'Validation Method' is set to 'Custom Query'",
      "uppercaseColumns": "Defines if the target table should have all it's column in uppercase or not. \n\nAuto means that depening on what the target database type is, it will set upper or lowercase automatically\nOracle - Uppercase\nAll other - Lowercase",
      "truncateTarget": "Truncate the target table before we export the data. Not used by incremental exports",
      "mappers": "How many mappers should we use during export\nIf set to Auto, DBImport will calculate the number of mappers depening of last export size",
      "tableRowcount": "Information only\nNumber of rows in source table during last export",
      "targetRowcount": "Information only\nNumber of rows in target table during last export",
      "validationCustomQueryHiveValue": "Information only\nLast value return from the custom query on the source table",
      "validationCustomQueryTargetValue": "Information only\nLast value return from the custom query on the target table",
      "incrColumn": "The column in the source table that will be used to identify new rows for the incremental export. Must be a timestamp column",
      "incrValidationMethod": "How should we validate the incremental export?\n\nFull - validation will check to total number of rows up until max value and compare source with target.\nIncr - Only compare the rows between min and max value. Basically means what the incremental export wrote",
      "incrMinvalue": "Information only\nThe min value used by the last export",
      "incrMaxvalue": "Information only\nThe max value used by the last export",
	  "incrMinvaluePending": "Information only\nIf a value exists in this field, it means an incremental export is running or failed to run and this is the min value that was used",
      "incrMaxvaluePending": "Information only\nIf a value exists in this field, it means an incremental export is running or failed to run and this is the max value that was used",
      "sqoopOptions": "Custom Options to send to sqoop command.",
	  "lastSize": "Information only\nThe size, in byte, of the amount of data that was transfered from source to target table in the last export",
      "lastRows": "Information only\nNumber of rows that was transfered from source to target table in the last export",
      "lastMappers": "Information only\nThe number of mappers used during the last export",
      "lastExecution": "Information only\"Last time sqoop export was executed",
      "javaHeap": "Sets hive.tez.container.size in the Hive session. This is an integer based value in MB. Should be a multiple of Yarn container size. If not set, then it will use the default specified in Yarn and TEZ\n\nExample: 3072 for 3G container size",
      "createTargetTableSql": "Information only\nSQL statement that was used to create the target table",
      "operatorNotes": "Free text field to write a note about the export.",
      "columns": {
        "columnName": "Information only\nName of column in target table",
        "columnType": "Information only\nColumn type in target table",
        "columnOrder": "Information only\nThe order of the column",
        "targetColumnName": "Set a custom name of the column",
        "targetColumnType": "Set a custom column type",
        "lastUpdateFromHive": "Information only\nTimestamp of last schema update from source table.",
        "includeInExport": "It's possible to include or exclude a column during export. That is controlled on this setting",
        "comment": "Information only\nThe column comment from the source system",
        "operatorNotes": "Free text field to write a note about the export."
      }
    }
  },
  "airflow": {
    "import": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute dag",
      "filterTable": "Filter string for database and table. ; separated. Wildcards (*) allowed. Example HIVE_DB.HIVE_TABLE; HIVE_DB.HIVE_TABLE",
      "finishAllStage1First": "true = All Import phase jobs will be completed first, and when all is successfull, the ETL phase start",
      "runImportAndEtlSeparate": "true = The Import and ETL phase will run in separate Tasks",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "retriesStage1": "Specific retries number for Import Phase",
      "retriesStage2": "Specific retries number for ETL Phase",
      "poolStage1": "Airflow pool used for stage1 tasks. NULL for the default Hostname pool",
      "poolStage2": "Airflow pool used for stage2 tasks. NULL for the default DAG pool",
      "operatorNotes": "Free text field to write a note about the import DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "autoRegenerateDag": "true = The DAG will be auto regenerated by manage command",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "metadataImport": "true = Run only getSchema and getRowCount, false = Run a normal import",
      "timezone": "Timezone used for schedule_interval column. Use full text timezone, example Europe/Stockholm",
      "email": "Email to send message to in case email_on_retry or email_on_failure is set to True",
      "emailOnFailure": "Send email on failures",
      "emailOnRetries": "Send email on retries",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "true = Use the retry_exponential_backoff Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, \nfalse = Run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "export": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute dag",
      "filterConnection": "Filter string for DBALIAS in export_tables",
      "filterTargetSchema": "Filter string for TARGET_SCHEMA  in export_tables",
      "filterTargetTable": "Filter string for TARGET_TABLE  in export_tables",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "operatorNotes": "Free text field to write a note about the export DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "autoRegenerateDag": "true = The DAG will be auto regenerated by manage command",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "timezone": "Timezone used for schedule_interval column. Use full text timezone, example Europe/Stockholm",
      "email": "Email to send message to in case email_on_retry or email_on_failure is set to True",
      "emailOnFailure": "Send email on failures",
      "emailOnRetries": "Send email on retries",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "true = Use the retry_exponential_backoff Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, \nfalse = Run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "custom": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute dag",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "operatorNotes": "Free text field to write a note about the custom DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "autoRegenerateDag": "true = The DAG will be auto regenerated by manage command",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "timezone": "Timezone used for schedule_interval column. Use full text timezone, example Europe/Stockholm",
      "email": "Email to send message to in case email_on_retry or email_on_failure is set to True",
      "emailOnFailure": "Send email on failures",
      "emailOnRetries": "Send email on retries",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "true = Use the retry_exponential_backoff Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, \nfalse = Run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "tasks": {
      "name": "Name of the Task in Airflow",
      "type": "The type of the Task",
      "placement": "Placement for the Task",
      "connection": "For  'JDBC SQL' Task Type, this specifies what database the SQL should run against",
      "airflowPool": "Airflow Pool to use",
      "airflowPriority": "Airflow Priority. Higher number, higher priority",
      "includeInAirflow": "Enable or disable the Task in the DAG during creation of DAG file",
      "taskDependencyDownstream": "Defines the upstream dependency for the Task. Comma separated list",
      "taskDependencyUpstream": "Defines the upstream dependency for the Task. Comma separated list",
      "taskConfig": "The configuration for the Task. Depends on what Task type it is",
      "sensorPokeInterval": "Poke interval for sensors in seconds",
      "sensorTimeoutMinutes": "Timeout for sensors in minutes",
      "sensorConnection": "Name of Connection in Airflow",
      "sensorSoftFail": "Setting this to 1 will add soft_fail=True on sensor",
      "sudoUser": "The task will use this user for sudo instead of default"
    }
  },
  "connection": {
    "name": "Name of the Database connection",
    "connectionString": "The JDBC URL String",
    "privateKeyPath": "",
    "publicKeyPath": "",
    "credentials": "Encrypted fields for credentials.m Changed by the saveCredentialTool",
    "source": "This value will come in the dbimport_source column if present. \nPriority is table, connection",
    "forceString": "If set to 1, all character based fields (char, varchar) will become string in Hive",
    "maxSessions": "You can limit the number of parallel sessions during import with this value. If NULL, then Max will come from configuration file",
    "createDatalakeImport": "If set to 1, the datalake_import column will be created on all tables that is using this dbalias",
    "timeWindowStart": "Start of the time window when we are allowed to run against this connection",
    "timeWindowStop": "End of the time window when we are allowed to run against this connection",
    "timeWindowTimezone": "Timezone used for timewindow_start and timewindow_stop columns. Use full text timezone, example Europe/Stockholm",
    "operatorNotes": "Free text field to write a note about the connection",
    "contactInformation": "Contact information. Used by Atlas integration",
    "description": "Description. Used by Atlas integration",
    "owner": "Owner of system and/or data. Used by Atlas integration",
    "environment": "Name of the Environment type",
    "seedFile": "File that will be used to fetch the custom seed that will be used for anonymization functions on data from the connection",
    "createForeignKey": "true = Create foreign keys, \nfalse = Dont create foreign keys",
    "atlasDiscovery": "true = Discover tables/views on this connection, \nfalse = Dont use Atlas discovery on this connection",
    "atlasIncludeFilter": "Include filter for Atlas discovery",
    "atlasExcludeFilter": "Exclude filter for Atlas discovery",
    "atlasLastDiscovery": ""
  }
}

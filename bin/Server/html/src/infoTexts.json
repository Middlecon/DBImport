{
  "table": {
    "import": {
      "database": "Name of the Database to import to",
      "table": "Name of the Table to import to",
      "connection": "Name of database connection. These are specified under 'Connection'",
      "sourceSchema": "Name of the schema in the remote database",
      "sourceTable": "Name of the table in the remote database",
      "importPhaseType": "What import method to use",
      "etlPhaseType": "What method to use for ETL phase",
      "importTool": "What tool should be used for importing data",
      "etlEngine": "What engine will be used to process etl stage",
      "lastUpdateFromSource": "Timestamp of last schema update from source. Fetching the schema is the first thing that happens during an import. During this part of the import, all columns are fetched and the list of columns will be updated. Before that, no columns exists for the table",
      "comment": "Information only\nThe table comment from the source system",
      "sourceTableType": "Information only\nType of table on the source system. Will show if this is a real table or a view",
      "importDatabase": "Override the database name used for the import table. Rules for default databases name is defined in 'Configurations'",
      "importTable": "Override the table name used for the import table. Rules for default table name is defined in 'Configurations'",
      "historyDatabase": "Override the database name used for the history table if that exists. Rules for default history databases name is defined in 'Configurations'",
      "historyTable": "Override the table name used for the history table if that exists. Rules for default history table name is defined in 'Configurations'",
      "airflowPriority": "This will set priority_weight in Airflow",
      "includeInAirflow": "Will the table be included in Airflow DAG when it matches the DAG selection",
      "operatorNotes": "Free text field to write a note about the import.",
      "validateImport": "Should the import be validated",
      "validationMethod": "Validation method to use\n\n'Row Count' - Will use the default method of counting the number of rows in both the source and target systems and compare the number. A certain diff can be allowed and this is defined in 'Allowed Validation Difference'\n\n'Custom Query' - Using this method will need two custom queries. One runs on the source system and one runs on the target system. Both queries must return one row with one column. The values in that single cell will be compared between the two queries. If they are the same, the validation is passed. You specify these custom queries in the 'Custom Query .... SQL' fields.",
      "validateSource": "There are two options for how to validate the source database.\n\n'Query before import' - This will run a 'select count(*) from ...' to get the number of rows in the source tablei before the actual import starts.\n\n'Imported rows' - Use the number of rows imported by the import tool as the number of rows in the source table\n\nUsing the imported rows function can be used to get a valid row amount for tables that is in constant change and will create a to big of a diff between the time for the query and the time for the import tool to execute.",
      "validateDiffAllowed": "How many rows that is allowed to diff. Setting this to Auto will make DBImport to auto calculated ithe allowed diff. If no diff is allowed at all, set this to 0",
      "sourceRowcount": "Information only\nThis is the amount of rows that the validation identified in the source system on the last full import",
      "sourceRowcountIncr": "Informatation only\nThis is the amount of rows that the validation identified in the source system on the last incremental import",
      "targetRowcount": "Informatation only\nThe number of rows that exists in the target table after the operation is executed",
      "validationCustomQuerySourceSQL": "Query to be executed on the source table. Only used when 'Validation Method' is set to 'Custom Query'",
      "validationCustomQueryHiveSQL": "Query to be executed on the target table. Only used when 'Validation Method' is set to 'Custom Query'",
      "validationCustomQueryValidateImportTable": "",
      "validationCustomQuerySourceValue": "Information only\nLast value return from the custom query on the source table",
      "validationCustomQueryHiveValue": "Information only\nLast value return from the custom query on the target table",
      "allowTextSplitter": "When primary key or key specified to get uniqe value is a text column, like char or varchar, the default is not to allow to use that for splits. Setting this to True will allow the splits on text columns. ",
      "forceString": "If set to True, all character based fields (char, varchar) will become string in target database. \n\nIf Default is selected, then the value will come from the settings on the connection used for this import.",
      "splitByColumn": "Column to split by when doing import with multiple sessions",
      "sqlWhereAddition": "Add a custom where clause into the query used to fetch the data from the source table. This is added AFTER the SQL WHERE. If it's an incr import, this will be after the incr limit statements. \n\nExample: orderId > 1000\n\nA special value can also be used here. If the string ${MAX_VALUE} exists in this field, the value returned from the query specified in 'Custom Max Query' will replace the text ${MAX_VALUE} with that value. This is used during very complex imports where you cant read all data in the table due to the status of the data.",
      "sqoopCustomQuery": "Use a custom query to read data from source table",
      "customMaxQuery": "You can use a custom SQL query that will get the Max value from the source database. This Max value will be used in an inremental import to know how much to read in each execution. Can also replace the string ${MAX_VALUE} specified under 'SQL WHERE Addition'",
      "sqoopUseGeneratedSql": "When an import is started, DBImport calculate and create a SQL statement that will be used to fetch all rows from the source table. Usually, this is fine and most import will use this generated SQL statement. But in some cases, you want to override the SQL statement and create your own. If you want to use your own, you specify this query in the 'Custom Query' field",
      "nomergeIngestionSqlAddition": "Will be added to the SQL WHERE statement that copy the data from the import to the target table. Will not affect the data imported from source table. Only active when a None-Merge imports is used (ETL Type not set to a Merge type). Usefull to filter out data",
      "sqoopOptions": "Custom Options to send to sqoop command.",
      "lastSize": "Information only\nThe size, in byte, of the amount of data that was transfered from source to import table in the last import",
      "lastRows": "Information only\nNumber of rows that was transfered from source to import table in the last import",
      "lastSqlSessions": "Information only\nThe number of SQL sessions used during the last import",
      "generatedHiveColumnDefinition": "Information only\nGenerated column definition for target table.",
      "generatedSqoopQuery": "Information only\nGenerated query for sqoop. This does not exist for Spark imports",
      "generatedSqoopOptions": "Information only\nGenerated options for sqoop command. This does not exist for Spark imports",
      "generatedPkColumns": "Information only\nGenerated Primary Keys.",
      "incrMode": "Incremental import mode\n\n'Last Modified - Will use a DATE or TIMESTAMP column to determine what rows to read incrementally\nAppend - Will use an integer based (INT, BIGINT..) column with increasing values to determine what rows to read incrementally",
      "incrColumn": "What column to use to identify new rows",
      "incrValidationMethod": "How should we validate the incremental import?\n\nFull - validation will check to total number of rows up until maxvalue and compare source with target.\nIncr - Only compare the rows that was imported",
      "incrMinvalue": "Information only\nThe min value used by the last import",
      "incrMaxvalue": "Information only\nThe max value used by the last import",
      "incrMinvaluePending": "Information only\nIf a value exists in this field, it means an incremental import is running or failed to run and this is the min value that was used",
      "incrMaxvaluePending": "Information only\nIf a value exists in this field, it means an incremental import is running or failed to run and this is the max value that was used",
      "createForeignKeys": "Should the import try to create the same Foreign Keys in the target databas as it exists on the source database? This can be specified here on the table or as a defalt value on the connection used. \nIf the foreign table does not exists in the target database, a warning text will be printed during import that the table is missing, but the import will still continue to import the data.",
      "invalidateImpala": "If Impala is used, you can set this to True and an 'invalidate metadata' query is executed in Impala at the end of the import to make sure the last imported data is visable. The default value is set under 'Configuration'",
      "softDeleteDuringMerge": "When set to True, the row will be marked as deleted instead of actually being removed from the table. Only used for Merge imports",
      "pkColumnOverride": "Force the import and target table to define another PrimaryKey constraint. Comma separeted list of columns",
      "pkColumnOverrideMergeonly": "Force the import to use another PrimaryKey constraint during Merge operations. Comma separeted list of columns",
      "mergeCompactionMethod": "Compaction method to use after import using merge methods is completed. Default means a major compaction if it is configured to do so in the configuration table",
      "datalakeSource": "This value will come in the dbimport_source column if present. Overrides the same setting in jdbc_connections table",
      "sqlSessions": "Auto or positiv number for a fixed number of SQL sessions that will be used in parallell against the source database. If Auto, then it's calculated based of last import size. \nIf the import is having problems to split the data, a common option is to set this to 1 to avoid splits all together",
      "splitCount": "Sets tez.grouping.split-count in the Hive session",
      "hiveContainerSize": "Sets hive.tez.container.size in the Hive session. This is an integer based value in MB. Should be a multiple of Yarn container size. If not set, then it will use the default specified in Yarn and TEZ\n\nExample: 3072 for 3G container size",
      "sparkExecutorMemory": "Memory for each Spark executor. Overrides default value set in dbimport.cfg\n\nExample: 32G or 2688M",
      "sparkExecutors": "If Spark Dynamic Allocation is set to True in dbimport.cfg, this configuration option will control the maximum dynamic number of Spark executors to use (spark.dynamicAllocation.maxExecutors).\n\nIf Spark Dynamic Allocation is False, this will set the max executors to use for the entire Spark session (spark.executor.instances).\n\nThis should not be mixed up with how many sessions that will be used when transfering data. That is still controlled by 'SQL Sessions' setting and limited by Import Max Sessions in Configuration/Global.\n\nOverrides the default value in Configuration/Global",
      "copyFinished": "Information only\nTime when last copy from Master DBImport instance was completed.",
      "copySlave": "Defines if this table is a Master table or a Slave table. If an import from the Master DBImport comes, this will be set to Slave automatically by DBImport. But in order to switch back, you need to manually set this to False.",
      "columns": {
        "columnName": "Information only\nName of column in target database This might not be the same as the column name in the source system due to special character handling",
        "columnOrder": "Information only\nOrder of the column",
        "sourceColumnName": "Information only\nName of the column in the source system. This might not be the same as the column name in the target system due to special character handling",
        "columnType": "Information only\nColumn type in target database. This might not be the same as the column type in source system due to support for different column types in different database systems.",
        "sourceColumnType": "Information only\nColumn type in srouce database. This might not be the same as the column type in target system due to support for different column types in different database systems.",
        "sourceDatabaseType": "Information only\nThe type of database",
        "columnNameOverride": "Set a custom name of the column",
        "columnTypeOverride": "Set a custom column type",
        "sqoopColumnType": "Used to create a correct --map-column-java setting for sqoop.",
        "sqoopColumnTypeOverride": "Set the --map-column-java field to a fixed value and not calculated by DBImport",
        "forceString": "If set to True, all character based fields (char, varchar) will become string in Hive. Overrides the same setting on the table or the connection",
        "includeInImport": "It's possible to include or exclude a column during import. That is controlled on this setting ",
        "sourcePrimaryKey": "Information only\nIf the column is part of the primary key, this indicates the order of the column in the primary key",
        "lastUpdateFromSource": "Timestamp of last schema update from source",
        "comment": "Information only\nThe column comment from the source system",
        "operatorNotes": "Free text field to write a note about the column",
        "anonymizationFunction": "What anonymization function should be used with the data in this column"
      }
    },
    "export": {
      "connection": "Database connection name that we export to",
      "targetSchema": "Schema on the target system",
      "targetTable": "Table on the target system",
      "exportType": "What export method to use.",
      "exportTool": "What tool should be used for exporting data.",
      "database": "Name of the database that we export from",
      "table": "Name of the table that we export from",
      "lastUpdateFromHive": "Timestamp of last schema update from source system. Fetching the schema is the first thing that happens during an export. During this part of the export, all columns are fetched and the list of columns on the export table will be updated. Before that, no columns exists for the table ",
      "sqlWhereAddition": "Add a custom where clause into the query used to fetch the data from the source table. This is added AFTER the SQL WHERE. If it's an incr export, this will be after the incr limit statements. \n\nExample: orderId > 1000",
      "includeInAirflow": "Will the table be included in Airflow DAG when it matches the DAG selection",
      "airflowPriority": "This will set priority_weight in Airflow",
      "forceCreateTempTable": "Force export to create a table and export that instead. Useful when exporting views",
      "validateExport": "Should the export be validated?",
      "validationMethod": "Validation method to use\n\n'Row Count' - Will use the default method of counting the number of rows in both the source and target systems and compare the number. \n\n'Custom Query' - Using this method will need two custom queries. One runs on the source system and one runs on the target system. Both queries must return one row with one column. The values in that single cell will be compared between the two queries. If they are the same, the validation is passed. You specify these custom queries in the 'Custom Query .... SQL' fields.",
      "validationCustomQueryHiveSQL": "Query to be executed on the source table. Only used when 'Validation Method' is set to 'Custom Query'",
      "validationCustomQueryTargetSQL": "Query to be executed on the target table. Only used when 'Validation Method' is set to 'Custom Query'",
      "uppercaseColumns": "Defines if the target table should have all it's column in uppercase or not. \n\nAuto means that depening on what the target database type is, it will set upper or lowercase automatically\nOracle - Uppercase\nAll other - Lowercase",
      "truncateTarget": "Truncate the target table before we export the data. Not used by incremental exports",
      "sqlSessions": "How many SQL sessions that will be used in parallell against the target database\nIf set to Auto, DBImport will calculate the number of SQL sessions depening of last export size",
      "tableRowcount": "Information only\nNumber of rows in source table during last export",
      "targetRowcount": "Information only\nNumber of rows in target table during last export",
      "validationCustomQueryHiveValue": "Information only\nLast value return from the custom query on the source table",
      "validationCustomQueryTargetValue": "Information only\nLast value return from the custom query on the target table",
      "incrColumn": "The column in the source table that will be used to identify new rows for the incremental export. Must be a timestamp column",
      "incrValidationMethod": "How should we validate the incremental export?\n\nFull - validation will check to total number of rows up until max value and compare source with target.\nIncr - Only compare the rows between min and max value. Basically means what the incremental export wrote",
      "incrMinvalue": "Information only\nThe min value used by the last export",
      "incrMaxvalue": "Information only\nThe max value used by the last export",
      "incrMinvaluePending": "Information only\nIf a value exists in this field, it means an incremental export is running or failed to run and this is the min value that was used",
      "incrMaxvaluePending": "Information only\nIf a value exists in this field, it means an incremental export is running or failed to run and this is the max value that was used",
      "sqoopOptions": "Custom Options to send to sqoop command.",
      "lastSize": "Information only\nThe size, in byte, of the amount of data that was transfered from source to target table in the last export",
      "lastRows": "Information only\nNumber of rows that was transfered from source to target table in the last export",
      "lastSqlSessions": "Information only\nThe number of SQL sessions used during the last export",
      "lastExecution": "Information only\"Last time sqoop export was executed",
      "hiveContainerSize": "Sets hive.tez.container.size in the Hive session. This is an integer based value in MB. Should be a multiple of Yarn container size. If not set, then it will use the default specified in Yarn and TEZ\n\nExample: 3072 for 3G container size",
      "createTargetTableSql": "Information only\nSQL statement that was used to create the target table",
      "operatorNotes": "Free text field to write a note about the export.",
      "columns": {
        "columnName": "Information only\nName of column in target table",
        "columnType": "Information only\nColumn type in target table",
        "columnOrder": "Information only\nThe order of the column",
        "targetColumnName": "Set a custom name of the column",
        "targetColumnType": "Set a custom column type",
        "lastUpdateFromHive": "Information only\nTimestamp of last schema update from source table.",
        "includeInExport": "It's possible to include or exclude a column during export. That is controlled on this setting",
        "comment": "Information only\nThe column comment from the source system",
        "operatorNotes": "Free text field to write a note about the export."
      }
    }
  },
  "airflow": {
    "import": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute DAG. This can be a cron entry (single quoted), a time or 'None' to not schedule it at all. The time is for a specific timezone. There is a default under configuration or you can override it in the DAG with the 'Timezone' option.\n\nExamples:\n21:00\nNone\n'*/10 6-22 * * *'",
      "filterTable": "Semi-colon seperated filter for import databases and tables (seperated with a .) that will be included in this DAG. Wildcards (*) allowed.\n\nExamples\ndatabase1.table1;database1.table2\ndatabase2:*",
      "finishAllStage1First": "The import can be split into two parts. One for the import stage that will fetch the data from the source system, validate it and store it in the cluster and the ETL stage that loads the data from the imported files into the target tables. Normally these are executed together, but in certain cases it's necessary to make sure all tables are loaded to the filesystem as fast as possible and take the slower ETL part at a later stage. This will reduce the time between when the load starts and stop on the source system. Useful for examples when we are only allowed to load the data during a limited timewindow from source system",
      "runImportAndEtlSeparate": "The import can be split into two parts. One for the import stage that will fetch the data from the source system, validate it and store it in the cluster and the ETL stage that loads the data from the imported files into the target tables. Some use-cases want to add tasks that triggers once the import part is completed, and setting this to True is the way to make that possible",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "retriesStage1": "Specific retries number for Import Phase",
      "retriesStage2": "Specific retries number for ETL Phase",
      "poolStage1": "Airflow pool used for stage1 tasks. Overrides the dafault pool name",
      "poolStage2": "Airflow pool used for stage2 tasks. Overrides the dafault pool name",
      "operatorNotes": "Free text field to write a note about the import DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "autoRegenerateDag": "The DAG will be auto regenerated by manage command and stored in the Airflow DAG directory.",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "metadataImport": "If this is set to True, only the metadata (columns, keys, indexes) is fetched from source system and not the data. Used to get the list of columns into the import defenition before the first real import",
      "timezone": "Timezone used for the time specified in the 'Schedule Interval' column. Use full text timezone. \n\nExample: Europe/Stockholm",
      "email": "Email addresses to send message to for 'Email on Failure' and 'Email on Retries'",
      "emailOnFailure": "Should Airflow send an email during failures or not?",
      "emailOnRetries": "Should Airflow send an email during retries or not?",
      "tags": "Comma seperated list of Airflow tags that will be set on the DAG",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "Use the 'retry_exponential_backoff' Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, \nIf this is not used, the retries will run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "export": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute DAG. This can be a cron entry (single quoted), a time or 'None' to not schedule it at all. The time is for a specific timezone. There is a default under configuration or you can override it in the DAG with the 'Timezone' option.\n\nExamples:\n21:00\nNone\n'*/10 6-22 * * *'",
      "filterConnection": "Filter string for 'Connections' in the exports",
      "filterTargetSchema": "Filter string for the schema in the exports",
      "filterTargetTable": "Filter string for table  in the exports",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "operatorNotes": "Free text field to write a note about the export DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "autoRegenerateDag": "The DAG will be auto regenerated by manage command and stored in the Airflow DAG directory.",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "timezone": "Timezone used for the time specified in the 'Schedule Interval' column. Use full text timezone. \n\nExample: Europe/Stockholm",
      "email": "Email addresses to send message to for 'Email on Failure' and 'Email on Retries'",
      "emailOnFailure": "Should Airflow send an email during failures or not?",
      "emailOnRetries": "Should Airflow send an email during retries or not?",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "Use the 'retry_exponential_backoff' Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, \nIf this is not used, the retries will run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "custom": {
      "name": "Name of Airflow DAG",
      "scheduleInterval": "Time to execute DAG. This can be a cron entry (single quoted), a time or 'None' to not schedule it at all. The time is for a specific timezone. There is a default under configuration or you can override it in the DAG with the 'Timezone' option.\n\nExamples:\n21:00\nNone\n'*/10 6-22 * * *'",
      "retries": "How many retries should the Task do in Airflow before it fails",
      "operatorNotes": "Free text field to write a note about the custom DAG",
      "applicationNotes": "Free text field that can be used for application documentaton, notes or links",
      "airflowNotes": "Documentation that will be available in Airflow. Markdown syntax",
      "autoRegenerateDag": "The DAG will be auto regenerated by manage command and stored in the Airflow DAG directory.",
      "sudoUser": "All tasks in DAG will use this user for sudo instead of default",
      "timezone": "Timezone used for the time specified in the 'Schedule Interval' column. Use full text timezone. \n\nExample: Europe/Stockholm",
      "email": "Email addresses to send message to for 'Email on Failure' and 'Email on Retries'",
      "emailOnFailure": "Should Airflow send an email during failures or not?",
      "emailOnRetries": "Should Airflow send an email during retries or not?",
      "tags": "Comma seperated list of Airflow tags that will be set on the Dag",
      "slaWarningTime": "Maximum time this DAG should run before Airflow triggers a SLA miss",
      "retryExponentialBackoff": "Use the 'retry_exponential_backoff' Airflow function that will cause the retry between failed tasks to be longer and longer each time instead of a fixed time, \nIf this is not used, the retries will run with a fixed time of 5 min between the task retries",
      "concurrency": "Set the max number of concurrent tasks in the DAG while executing. Overrides the default value specified in Airflow configuration"
    },
    "tasks": {
      "name": "Name of the Task in Airflow",
      "type": "The type of the Task\n\nshell script: This will execute a script on the server\nHive SQL: Will use the manage command to start a Hive SQL\nHive SQL Script: Will use the manage command to start a Hive SQL script\nTrigger DAG: This will trigger the start of another Airflow DAG\nDAG Sensor: Will create a 'wait' task in Airflow that will wait until another DAG is finished\nSQL Sensor: Will execute a SQL script against an external database to check for status before continue the DAG flow\nDBImport command: Start a DBImport command manually. Can be a import, export or manage command",
      "placement": "The DBImport Airflow DAGs is seperated into 3 parts. The default part that always exist is the Main part. This is where all the import or exports are executed. If you want to execute a custom task before all the import starts, you are 'before main'. If you want to execute a task at the end once all the imports are finnished, you are 'after main'. \n\nBefore main is very often used for DAG sensors to make sure the DAG isnt started until another DAG is finished, or a SQL Sensor to make sure the data is ready to be imported. After main is often used to start another DAG or write SQL data into an external database to notice them that the job is finished",
      "connection": "For  'JDBC SQL' Task Type, this specifies what database the SQL should run against",
      "airflowPool": "Airflow Pool to use",
      "airflowPriority": "Airflow Priority. Higher number, higher priority",
      "includeInAirflow": "Enable or disable the Task in the DAG during creation of DAG file",
      "taskDependencyDownstream": "Defines the downstream dependency for the Task. Comma separated list",
      "taskDependencyUpstream": "Defines the upstream dependency for the Task. Comma separated list",
      "taskConfig": "The configuration for the Task. Depends on what Task type it is\n\nshell script: Full path with arguments to the script that will be executed\nHive SQL: The full Hive SQL query\nHive SQL Script: Full path to the file containing the Hive SQL queries\nTrigger DAG: Name of Airflow DAG to start\nDAG Sensor: Name of the DAG and Task, seperated with a . (dot) of the task to wait for.\nSQL Sensor: The SQL command to execute against the 'Sensor Connection' Airflow connection\nDBImport command: The full DBImport command without the path. Start with example import or export",
      "sensorPokeInterval": "Poke interval for sensors in seconds",
      "sensorTimeoutMinutes": "Timeout for sensors in minutes",
      "sensorConnection": "Name of Connection in Airflow to use when 'Type' is 'SQL Sensor'",
      "sensorSoftFail": "If set to True, the sensor task is marked as skipped if the condition is not met by the timeout.",
      "sudoUser": "The task will use this user for sudo instead of default"
    }
  },
  "connection": {
    "name": "Name of the Database connection",
    "connectionString": "The JDBC URL String",
    "privateKeyPath": "Full path name to a custom private key that will be used to encrypt the credentials for this connection",
    "publicKeyPath": "Full path name to a custom public key that will be used to encrypt the credentials for this connection",
    "credentials": "Encrypted fields for credentials. Changed by the 'manage --encryptCredentials' tool",
    "source": "This value will come in the dbimport_source column if present. \nCan also be set in import tables and that have a higher priority than setting it here on the connection",
    "forceString": "If True, all character based fields (char, varchar) will become 'string' in target system",
    "maxSessions": "You can limit the number of parallel sessions during import with this value. If left blank, the max will come from the global configuration",
    "createDatalakeImport": "If set to true, a column will be created in the table called 'datalake_import' with the timestamp of when the data was imported into the table",
    "timeWindowStart": "Start of the time window when we are allowed to run against this connection",
    "timeWindowStop": "End of the time window when we are allowed to run against this connection",
    "timeWindowTimezone": "Timezone used for timewindow_start and timewindow_stop columns. Use full text timezone\n\nExample: Europe/Stockholm",
    "operatorNotes": "Free text field to write a note about the connection",
    "contactInformation": "Contact information. Used by Atlas integration",
    "description": "Description. Used by Atlas integration",
    "owner": "Owner of system and/or data. Used by Atlas integration",
    "environment": "Name of the Environment type",
    "seedFile": "Full path to file that will be used to fetch the custom seed that will be used for anonymization functions on data from the connection",
    "createForeignKey": "Should the Foreign Keys be created in the target system or not?",
    "atlasDiscovery": "Should we run the Atlas discovery to populate Atlas metadata with source system information, regardless if we import it or not. Requires a seperate DBImport daemon to be running on the server",
    "atlasIncludeFilter": "Include filter for Atlas discovery",
    "atlasExcludeFilter": "Exclude filter for Atlas discovery",
    "atlasLastDiscovery": "Timestamp of last discovery process for Atlas"
  },

  "configuration": {
    "global": {
      "airflow_aws_instanceids": "AWS Instance IDs to start the DBImport commands on. Comma-separated list of instance IDs. Tasks will be randomly assigned to one of the EMR clusters specified.",
      "airflow_aws_pool_to_instanceid": "When using AWS, should the Airflow pool be set to the same name as the instance id?",
      "airflow_create_pool_with_task": "True: The Airflow pool will be created with standard tasks.\n\nFalse: A direct database connection is required to add the pool through sql commands directly the airflow database. Connection details are available in dbimport.cfg for this.",
      "airflow_dag_directory": "Airflow path to DAG directory. When using AWS MWAA, this is the name of the S3 bucket.",
      "airflow_dag_file_group": "Group owner of created DAG file",
      "airflow_dag_file_permission": "File permission of created DAG file",
      "airflow_dag_staging_directory": "Airflow path to staging DAG directory",
      "airflow_dbimport_commandpath": "This is the path to DBImport. If sudo is required, this can be added here aswell. Use the variable ${SUDO_USER} instead of hardcoding the sudo username. Must end with a /\n\nExample\nsudo -iu ${SUDO_USER} /usr/local/dbimport/\n\nexport DBIMPORT_HOME=/usr/local/dbimport; export HADOOP_CONF_DIR=/etc/hadoop/conf; source /home/dbimport/python/bin/activate; /usr/local/dbimport/",
      "airflow_default_pool_size": "If DBImport creates the Airflow pool, this is the initial size of it",
      "airflow_disable": "If True, you will disable all DBImport executions from Airflow. This is what the 'start' Task is looking at",
      "airflow_dummy_task_queue": "Queue to use for dummy tasks (stop, stage_one_complete and more)",
      "airflow_major_version": "What is the major version of Airflow? 1 or 2 is valid options. Controls how the DAG files are generated",
      "airflow_sudo_user": "What user will Airflow sudo to for executing DBImport. This value will replace the ${SUDO_USER} variable in 'DBImport CommandPath' setting",
      "airflow_url": "URL to Airflow WebUI. This will be used to create the link to the Airflow DAG direct from the DAG configuration page",
      "atlas_discovery_interval": "How many hours there should pass between each Atlas discovery of a jdbc connection",
      "cluster_name": "Name of Hadoop cluster. Same as 'dfs.nameservices' in hdfs-site.xml",
      "export_default_sessions": "How many SQL sessions should be used for tables that have never been exported before. After the initial export, DBImport will calculate based on previous export size how many sessions that will be used",
      "export_max_sessions": "The maximum number of SQL sessions to use during export",
      "export_stage_disable": "If True, you prevent new Export tasks from starting and running tasks will stop after the current stage is completed.",
      "export_staging_database": "Name of staging database to use during Exports",
      "export_start_disable": "If True, you prevent new Export tasks from starting. Running tasks will be completed",
      "hdfs_address": "Address to the hdfs filesystem\n\nExample\nhdfs://OnPrem-Test-01\nofs://OnPrem-Test-01",
      "hdfs_basedir": "Path to where on the hdfs filesystem dbimport will save it's files.\n\nExample\n/apps/dbimport",
      "hdfs_blocksize": "The HDFS blocksize in bytes. This is the same as 'dfs.blocksize' in hdfs-site.xml",
      "hive_acid_with_clusteredby": "If True, then ACID tables will be created with a clustered by option based on the PK. Should not be used with Hive3 and later",
      "hive_insert_only_tables": "If True, then the non-merge tables in Hive will be ACID insert-only. Should not be used with Hive3 and later",
      "hive_major_compact_after_merge": "If True, DBImport will run a major compaction after the merge operations is completed",
      "hive_print_messages": "If True, Hive will print additional messages during SQL operations",
      "hive_remove_locks_by_force": "If True, DBImport will remove Hive locks before import by force. This is done directly against the Hive metastore SQL database. Connection details are available in dbimport.cfg for this",
      "hive_validate_before_execution": "If True, DBImport will run a group by query against the validate table and verify the result against reference values hardcoded in DBImport",
      "hive_validate_table": "The table to run the validate query against",
      "impala_invalidate_metadata": "If True, then DBImport will connect to Impala and invalidate or refresh the metadata after import is completed",
      "import_columnname_delete": "Column name for timestamp column with delete date that is added to table during merge operations. Only exists if soft-delete is enabled for the import",
      "import_columnname_histtime": "Column name for timestamp column added to history table to contain information about when the insert, update or delete operation was done",
      "import_columnname_import": "Column name for timestamp column added to table during none-merge operations",
      "import_columnname_insert": "Column name for timestamp column with insert time that is added to table during merge operations",
      "import_columnname_iud": "Column name for char(1) column added to table during merge operations with information if the last operation is an insert, update or delete",
      "import_columnname_source": "Column name for source information if that is configured for the Connection",
      "import_columnname_update": "Column name for timestamp column with update time that is added to table during merge operations",
      "import_default_sessions": "How many SQL sessions should be used for tables that have never been imported before. After the initial import, DBImport will calculate based on previous import size how many sessions that will be used",
      "import_history_database": "Name of history database to use during Imports. {DATABASE} is supported within the name and will be replaced during import with the name of the import database.",
      "import_history_table": "Name of history table to use during Imports. Both {DATABASE} and {TABLE} is supported within the name and will be replaced during import with the name of the import database and table.  Both {DATABASE} and {TABLE} must be used once, but {DATABASE} could be specified in the 'import_history_database' setting instead",
      "import_max_sessions": "The maximum number of SQL sessions to use during imports",
      "import_process_empty": "If True, then the import will do a full processing of import even if they contain no data.",
      "import_stage_disable": "If True, you prevent new tasks from starting and running Import tasks will stop after the current stage is completed.",
      "import_staging_database": "Name of staging database to use during Imports. {DATABASE} is supported within the name and will be replaced during import with the name of the import database.",
      "import_staging_table": "Name of staging table to use during Imports. Both {DATABASE} and {TABLE} is supported within the name and will be replaced during import with the name of the import database and table.  Both {DATABASE} and {TABLE} must be used once, but {DATABASE} could be specified in the 'import_staging_database' setting instead",
      "import_start_disable": "If True, you prevent new Import tasks from starting. Running tasks will be completed",
      "import_work_database": "Name of work database to use during Imports. This is used for internal tables and should not be accessed by end-users. {DATABASE} is supported within the name and will be replaced during import with the name of the import database.",
      "import_work_table": "Prefix-name of work tables to use during Imports. These tables is used for internal DBImport tables and should not be access by end-users. Both {DATABASE} and {TABLE} is supported within the name and will be replaced during import with the name of the import database and table.  Both {DATABASE} and {TABLE} must be used once, but {DATABASE} could be specified in the 'import_work_database' setting instead",
      "kafka_brokers": "Comma separeted list of Kafka brokers.\n\nExample\nserver1:6667,server2:6667,server3:6667",
      "kafka_saslmechanism": "Kafka SASL mechanism",
      "kafka_securityprotocol": "Kafka Security Protocol",
      "kafka_topic": "Kafka topic to send the data to",
      "kafka_trustcafile": "Kafka CA Trust file for SSL",
      "post_airflow_dag_operations": "Post start and stop activities for Airflow DAGs to enabled receivers",
      "post_data_to_kafka_extended": "Enable extended statistics in Kafka data",
      "post_data_to_kafka": "Enable the Kafka endpoint to be able to receive information regarding completed imports and exports",
      "post_data_to_rest_extended": "Enable extended statistics in the REST data",
      "post_data_to_rest": "Enable the REST endpoint to be able to receive information regarding completed imports and exports",
      "post_data_to_awssns_extended": "Enable extended statistics in AWS SNS data",
      "post_data_to_awssns": "Enable the AWS SNS endpoint to be able to receive information regarding completed imports and exports",
      "post_data_to_awssns_topic": "Full ARN to AWS SNS Topic where the json message will be sent\n\nExample\narn:aws:sns:region:account:topic",
      "restserver_admin_user": "Username of administrator on the REST server with all permission",
      "restserver_authentication_method": "REST server authentication method. \nThe following options are supported: \nlocal\npam",
      "restserver_token_ttl": "Time-to-live, in minutes, that the oauth2 tokens are valid",
      "rest_timeout": "Timeout for the REST call",
      "rest_trustcafile": "REST CA Trust file for SSL",
      "rest_url": "Rest server URL",
      "rest_verifyssl": "If True, the SSL communication should be fully validated with a correct trust certificate. You need to specify a valid trustfile for this",
      "spark_max_executors": "If Spark Dynamic Allocation is set to True in dbimport.cfg, this configuration option will control the maximum dynamic number of Spark executors to use (spark.dynamicAllocation.maxExecutors).\n\nIf Spark Dynamic Allocation is False, this will set the max executors to use for the entire Spark session (spark.executor.instances).\n\nThis should not be mixed up with how many sessions that will be used when transfering data. That is still controlled by Import/Export Max Sessions",
      "timezone": "The default timezone that the configured times are meant for. Use full text timezone. \n\nExample: Europe/Stockholm"
    },
    "jdbcDrivers": {
      "databaseType": "Database type. This in combination with version creates a uniqe value that the jdbc connection string uses to find the correct driver and classpath",
      "version": "Version of the driver type. This is used by DBImport if different versions of the driver exists for the same database type. Has nothing to do with version of driver itself.",
      "driver": "Java class for JDBC driver",
      "classpath": "Full path to JDBC driver/jar file. If more than one file is required, separate them with : and no spaces"
    }
  },
  "discover": {
    "import": {
      "connection": "",
      "schemaFilter": "",
      "tableFilter": "",
      "database": "",
      "addSchemaToTable": "",
      "addCounterToTable": "",
      "addCustomText": ""
    },
    "export": {
      "connection": "",
      "databaseFilter": "",
      "tableFilter": "",
      "targetSchema": "",
      "addDBToTable": "",
      "addCounterToTable": "",
      "addCustomText": ""
    }
  }
}

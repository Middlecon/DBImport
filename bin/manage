#!/usr/bin/env python3
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import sys
import getopt
import logging
from common import constants as constant
from common.Exceptions import *
from DBImportConfig import common_config
from DBImportConfig import import_config
# from DBImportConfig import rest
from DBImportOperation import common_operations
from DBImportOperation import import_operations
from DBImportOperation import export_operations
from DBImportOperation import copy_operations
from DBImportOperation import atlas_operations
from Schedule import Airflow

def	printHeader():
	# Font created at http://patorjk.com/software/taag/#p=display&f=Big&t=DBImport%20-%20setup
	sys.stdout.write(u"\u001b[35m")  # Magenta
	sys.stdout.flush()
	print("")
	print(" _____  ____ _____                            _              __  __                               ")
	print("|  __ \|  _ \_   _|                          | |            |  \/  |                              ")
	print("| |  | | |_) || |  _ __ ___  _ __   ___  _ __| |_   ______  | \  / | __ _ _ __   __ _  __ _  ___  ")
	print("| |  | |  _ < | | | '_ ` _ \| '_ \ / _ \| '__| __| |______| | |\/| |/ _` | '_ \ / _` |/ _` |/ _ \ ")
	print("| |__| | |_) || |_| | | | | | |_) | (_) | |  | |_           | |  | | (_| | | | | (_| | (_| |  __/ ")
	print("|_____/|____/_____|_| |_| |_| .__/ \___/|_|   \__|          |_|  |_|\__,_|_| |_|\__,_|\__, |\___| ")
	print("                            | |                                                        __/ |      ")
	print("                            |_|                                                       |___/       ")
	sys.stdout.write(u"\u001b[0m")  # Reset
	sys.stdout.flush()
	print("")
	print("Version: %s"%(constant.VERSION))
	print("")

def printMainHelp():
	printHeader()
	print ("Options:")
	print ("  --clearImportStage                Clear an Import stage.")
	print ("  --clearExportStage                Clear an Export stage.")
	print ("  --testConnection                  Connect to the [Connection] and verifies if we can connect successfully")
	print ("  --encryptCredentials              Encrypt username and password for a Database Connection")
	print ("  --encryptInstance                 Encrypt username and password for a DBImport Instance")
	print ("  --encryptString                   Encrypt a string. Used for encryption of configuration in config file")
#	print ("  --sendJSONdata                    Posts the JSON data in the 'json_to_send' table to the configured destinations")
	print ("  --repairIncrementalImport         Repairs an incremental import that is out-of-sync with source system")
	print ("  --repairAllIncrementalImports     Repairs all incremental import that have an active stage.")
	print ("  --resetIncrementalImport          Resets an incremental import. This will truncate the Hive table")
	print ("  --resetCDCImport                  Resets an CDC import that is out-of-sync with source system. This will truncate the Hive table")
	print ("  --repairIncrementalExport         Repairs an incremental export that is out-of-sync with target system")
	print ("  --resetIncrementalExport          Resets an incremental export. Will truncate the target table")
	print ("  --dropExportTable                 Drops an Export table on target system")
	print ("  --addImportTable                  Connects to the source database and reads tables and view")
	print ("  --addExportTable                  Add export definitions for tables in Hive")
	print ("  --checkAirflowExecution           Checks if it's allowed to start Airflow DAG's or not")
	print ("  --sendAirflowStopMessage          Sends a message to Kafka and/or Rest saying that the specified Airflow DAG have finished")
	print ("  --airflowGenerate                 Generate Airflow DAG")
	print ("  --discoverAtlasRdbms              Discover all object on a Database Connection and update Atlas with the information")
	print ("  --runJDBCQuery                    Run a SQL command against a JDBC connection")
	print ("  --runHiveQuery                    Run a SQL command against Hive")
	print ("  --runHiveScript                   Run a SQL script against Hive")
	print ("")
	print ("General parameters to all commands:")
	print ("  -y, --yes          Auto answer 'Yes' to all questions")
	print ("  --help             Show this help message and exit")
	print ("  -v, --debug        Turn on extensive debug output. This will print passwords in cleartext, so use with caution")
	print ("  --quiet            Suppress logo and version output")

	sys.exit(1)

def print_clearImportStage_help():
	printHeader()
	print ("This will remove the staging information for a given import. The result will be that the next import will start")
	print ("from the begining regardless of where the previous import failed and stopped")
	print ("")
	print ("Required parameters:")
	print ("  -h [Hive Database], --hiveDB=[Hive Database]     Hive database")
	print ("  -t [Hive Table], --hiveTable=[Hive Table]        Hive table")
	print ("")
	sys.exit(1)

def print_clearExportStage_help():
	printHeader()
	print ("This will remove the staging information for a given export. The result will be that the next export will start")
	print ("from the begining regardless of where the previous export failed and stopped")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]  The alias of the JDBC connection")
	print ("  -S [Schema], --schema=[Schema]           Schema name of the table to export")
	print ("  -T [Table], --table=[Table]              Table to export")
	print ("")
	sys.exit(1)

def print_repairIncrementalImport_help():
	printHeader()
	print ("Repairs an incremental import by getting the max value for the incremental column in the source system and set that")
	print ("as a starting point for the next import.")
	print ("")
	print ("Required parameters:")
	print ("  -h [Hive Database], --hiveDB=[Hive Database]     Hive database")
	print ("  -t [Hive Table], --hiveTable=[Hive Table]        Hive table")
	print ("")
	sys.exit(1)

def print_resetIncrementalImport_help():
	printHeader()
	print ("Resets an incremental import. This will truncate the Target table in Hive and force the next import")
	print ("to start from the begining with a full import.")
	print ("")
	print ("Required parameters:")
	print ("  -h [Hive Database], --hiveDB=[Hive Database]     Hive database")
	print ("  -t [Hive Table], --hiveTable=[Hive Table]        Hive table")
	print ("")
	sys.exit(1)

def print_resetCDCImport_help():
	printHeader()
	print ("Resets an CDC import. This will truncate the Target table in Hive and force the next import")
	print ("to do an initial load with a full import.")
	print ("")
	print ("Required parameters:")
	print ("  -h [Hive Database], --hiveDB=[Hive Database]     Hive database")
	print ("  -t [Hive Table], --hiveTable=[Hive Table]        Hive table")
	print ("")
	sys.exit(1)

def print_repairIncrementalExport_help():
	printHeader()
	print ("Repairs an incremental export by reading the max value for the incremental column from the ")
	print ("Target Table and use that as the max value for the next export.")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]  The alias of the JDBC connection")
	print ("  -S [Schema], --schema=[Schema]           Schema name of the table to export")
	print ("  -T [Table], --table=[Table]              Table to export")
	print ("")
	sys.exit(1)

def print_resetIncrementalExport_help():
	printHeader()
	print ("Resets an incremental export. This will truncate the Target table and force the next export")
	print ("to start from the begining with a full export.")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]  The alias of the JDBC connection")
	print ("  -S [Schema], --schema=[Schema]           Schema name of the table to export")
	print ("  -T [Table], --table=[Table]              Table to export")
	print ("")
	sys.exit(1)

def print_dropExportTable_help():
	printHeader()
	print ("Drops an Export table and forces an incremental export to start from a full export.")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]  The alias of the JDBC connection")
	print ("  -S [Schema], --schema=[Schema]           Schema name of the table to export")
	print ("  -T [Table], --table=[Table]              Table to export")
	print ("")
	sys.exit(1)

def print_addImportTable_help():
	printHeader()
	print ("Connects to the source database and reads all tables and view, or a selection if [Schema] or [Table]")
	print ("is specified and insert those tables/views to import_tables.")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]        The alias of the JDBC connection")
	print ("  -h [Hive Database], --hiveDB=[Hive Database]   Hive database")
	print ("")
	print ("Optional parameters:")
	print ("  -S [Schema], --schema=[Schema]                 Filter for Schema name. Wildcard (*) is supported")
	print ("  -T [Table], --table=[Table]                    Filter for Table name. Wildcard (*) is supported")
	print ("  --addSchemaToTable                             The schema on the source database is added to the Hive Table name")
	print ("  --addCounterToTable                            A counter starting from [Start] is added to begining of the Table name")
	print ("  --counterStart=[Start]                         The number to start with when using --addCounterToTable")
	print ("  --addCustomText=[Custom Text]                  Add the custom text to the Table name")
	print ("  --jsonOutput                                   Only create a json output with tables that can be added")
	print ("")
	sys.exit(1)

def print_addExportTable_help():
	printHeader()
	print ("Add export definitions for all tables in Hive, or just specific tables if filters for ")
	print ("[Hive Database] or [Hive Table]")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]        The alias of the JDBC connection")
	print ("  -S [Schema], --schema=[Schema]                 The schema to create the tables in on the target system")
	print ("")
	print ("Optional parameters:")
	print ("  -h [Hive Database], --hiveDB=[Hive Database]   Filter for database name. Wildcard (*) is supported")
	print ("  -t [Hive Table], --hiveTable=[Hive Table]      Filter for Table name. Wildcard (*) is supported")
	print ("  --addDBToTable                                 The Hive database is added to the Target Table name")
	print ("  --addCounterToTable                            A counter starting from [Start] is added to begining of the Table name")
	print ("  --counterStart=[Start]                         The number to start with when using --addCounterToTable")
	print ("  --addCustomText=[Custom Text]                  Add the custom text to the Table name")
	print ("")
	sys.exit(1)

def print_testConnection_help():
	printHeader()
	print ("Connect to the [Connection] and verifies that DBImport can connect successfully")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]  The alias of the JDBC connection")
	print ("")
	sys.exit(1)

def print_discoverAtlasRdbms_help():
	printHeader()
	print ("Discover all object on a Database Connection and update Atlas with the information.")
	print ("")
	print ("Required parameters:")
	print ("  -a [Connection], --dbAlias=[Connection]  The alias of the JDBC connection")
	print ("")
	sys.exit(1)

def print_runJDBCQuery_help():
	printHeader()
	print ("Run the SQL defined in [SQL] against the specified [Connection].")
	print ("")
	print ("Required parameters:")
	print ("  --runJDBCQuery=[SQL]                     The SQL to execute")
	print ("  -a [Connection], --dbAlias=[Connection]  The alias of the JDBC connection")
	print ("")
	sys.exit(1)

def print_runHiveQuery_help():
	printHeader()
	print ("Run the SQL defined in [SQL] against Hive")
	print ("")
	print ("Required parameters:")
	print ("  --runHiveQuery=[SQL]       The SQL to execute")
	print ("")
	sys.exit(1)

def print_runHiveScript_help():
	printHeader()
	print ("Run the SQL's defined in [FILE]. This is actually executing beeline in the background and acts as a wrapper around it" )
	print ("")
	print ("Required parameters:")
	print ("  --runHiveScript=[FILE]                         File to execute SQL commands from")
	print ("")
	print ("Optional parameters:")
	print ("  -h [Hive Database], --hiveDB=[Hive Database]   Will add the database name to the connection string")
	print ("")
	sys.exit(1)

def print_checkAirflowExecution_help():
	printHeader()
	print ("Checks if the Airflow Execution is allowed or not. This is usually done first in each DAG and is used to globaly disable all DAG's that is controlled by DBImport")
	print ("")
	print ("Optional parameters:")
	print ("  --airflowDAG=[DAG Name]              If this is supplied and the post_to_kafka or post_to_rest is enabled, a message will be posted saying that this DAG started")
	print ("")
	sys.exit(1)

def print_sendAirflowStopMessage_help():
	printHeader()
	print ("Sends a message to Kafka and/or Rest saying that the specified Airflow DAG have finished")
	print ("")
	print ("Required parameters:")
	print ("  --airflowDAG=[DAG Name] ")
	print ("")
	sys.exit(1)

def print_airflow_help():
	printHeader()
	print ("Generate the Airflow DAG file. If no DAG is specified, all configured DAG's are generated" )
	print ("")
	print ("Required parameters:")
	print ("  --airflowGenerate             Generate the Airflow DAG file. If no DAG is specified, all configured DAG's are generated")
	print ("")
	print ("Optional parameters:")
	print ("  --airflowDAG=[DAG name]       Name of the DAG to generate. Cant be specified at the same time as --airflowAutoDAGonly")
	print ("  --airflowAutoDAGonly          Will only generate those DAGs that is marked as 'auto_regenerate_dag = 1'")
	print ("  -w, --airflowWriteDAG         If specified, the DAG is created directly in Airflow. If not, it's created in the temp folder")
	print ("")
	sys.exit(1)

def main(argv):
	try:
		opts, args = getopt.getopt(argv, "?yvwh:t:a:S:T:", ["quiet", "yes", "help", "resetIncrementalExport", "repairAllIncrementalImports", "repairIncrementalImport", "resetIncrementalImport", "resetCDCImport", "sendJSONstatistics", "encryptCredentials", "clearImportStage", "Hive_DB=", "hiveDB=", "Hive_Table=", "hiveTable=", "dbAlias=", "schema=", "table=", "debug", "repairIncrementalExport", "clearExportStage", "dropExportTable", "addImportTable", "addSchemaToTable", "addExportTable", "addCounterToTable", "addDBToTable", "counterStart=", "addCustomText=", "testConnection", "airflowGenerate", "airflowAutoDAGonly", "airflowDAG=", "airflowDAGFolder=", "checkAirflowExecution", "runJDBCQuery=", "runHiveQuery=", "airflowWriteDAG", "runHiveScript=", "encryptString", "encryptInstance", "destination=", "discoverAtlasRdbms", "getAllConnectionPasswords", "sendAirflowStopMessage", "jsonOutput" ])
	except getopt.GetoptError:
		if "--runJDBCQuery" in argv:
			print_runJDBCQuery_help()
		if "--runHiveQuery" in argv:
			print_runHiveQuery_help()
		if "--runHiveScript" in argv:
			print_runHiveScript_help()
		else:
			printMainHelp()

	Hive_DB = None
	Hive_Table = None
	displayHelp = False
	operation = None
	loggingLevel = logging.INFO
	connectionAlias = None
	jdbcSchema = None
	jdbcTable = None
	jdbcQuery = None
	addSchemaToTable = False
	addCounterToTable = False
	addDBToTable = False
	addCustomText = None
	counterStart = None
	autoYesAnswer = False
	airflowAutoDAGonly = False
	airflowDAG = None
	airflowWriteDAG = False
	airflowDAGFolder=None
	copyDestination = None
	quietMode = False
	jsonOutput = False
	hiveScript = None
	commonConfig = None
	importConfig = None
	import_operation = None
	export_operation = None
	copy_operation = None

	if  len(opts) == 0:
		printMainHelp()

	for opt, arg in opts:
		if opt in ("-?", "--help"):
			displayHelp = True
		elif opt in ("-y", "--yes"):
			autoYesAnswer = True
		elif opt in ("-v", "--debug"):
			loggingLevel = logging.DEBUG
		elif opt in ("-h", "--hiveDB", "--Hive_DB"):
			Hive_DB = arg
		elif opt in ("-t", "--hiveTable", "--Hive_Table"):
			Hive_Table = arg
		elif opt in ("-a", "--dbAlias"):
			connectionAlias = arg
		elif opt in ("-S", "--schema"):
			jdbcSchema = arg
		elif opt in ("-T", "--table"):
			jdbcTable = arg
		elif opt == "--quiet":
			quietMode = True
		elif opt == "--jsonOutput":
			jsonOutput = True
		elif opt == "--addSchemaToTable":
			addSchemaToTable = True
		elif opt == "--addCounterToTable":
			addCounterToTable = True
		elif opt == "--addDBToTable":
			addDBToTable = True
		elif opt == "--addCustomText":
			addCustomText = arg
		elif opt == "--counterStart":
			counterStart = str(arg)
		elif opt == "--runJDBCQuery":
			jdbcQuery = str(arg)
			operation = "runJDBCQuery"
		elif opt == "--runHiveScript":
			hiveScript = str(arg)
			operation = "runHiveScript"
		elif opt == "--runHiveQuery":
			jdbcQuery = str(arg)
			operation = "runHiveQuery"
		elif opt == "--destination":
			copyDestination = str(arg)
		elif opt in ("-w", "--airflowWriteDAG"):
			airflowWriteDAG = True
		elif opt == "--airflowAutoDAGonly":
			airflowAutoDAGonly = True
		elif opt == "--airflowDAG":
			airflowDAG = str(arg)
		elif opt == "--airflowDAGFolder":
			airflowDAGFolder = str(arg)
		elif opt == "--airflowGenerate":
			operation = "airflowGenerate"
		elif opt == "--checkAirflowExecution":
			operation = "checkAirflowExecution"
		elif opt == "--sendAirflowStopMessage":
			operation = "sendAirflowStopMessage"
		elif opt == "--clearImportStage":
			operation = "clearImportStage"
		elif opt == "--clearExportStage":
			operation = "clearExportStage"
		elif opt == "--encryptString":
			operation = "encryptString"
		elif opt == "--encryptCredentials":
			operation = "encryptCredentials"
		elif opt == "--sendJSONstatistics":
			operation = "sendJSONstatistics"
		elif opt == "--repairIncrementalImport":
			operation = "repairIncrementalImport"
		elif opt == "--repairAllIncrementalImports":
			operation = "repairAllIncrementalImports"
		elif opt == "--resetIncrementalImport":
			operation = "resetIncrementalImport"
		elif opt == "--resetCDCImport":
			operation = "resetCDCImport"
		elif opt == "--resetIncrementalExport":
			operation = "resetIncrementalExport"
		elif opt == "--repairIncrementalExport":
			operation = "repairIncrementalExport"
#		elif opt == "--getAllConnectionPasswords":
#			operation = "getAllConnectionPasswords"
		elif opt == "--dropExportTable":
			operation = "dropExportTable"
		elif opt == "--addImportTable":
			operation = "addImportTable"
		elif opt == "--addExportTable":
			operation = "addExportTable"
		elif opt == "--testConnection":
			operation = "testConnection"
		elif opt == "--encryptInstance":
			operation = "encryptInstance"
		elif opt == "--discoverAtlasRdbms":
			operation = "discoverAtlasRdbms"


	if operation == None:
		printMainHelp()

	if operation == "clearImportStage" and (Hive_DB == None or Hive_Table == None or displayHelp == True):
		print_clearImportStage_help()

	if operation == "repairIncrementalImport" and (Hive_DB == None or Hive_Table == None or displayHelp == True):
		print_repairIncrementalImport_help()

	if operation == "resetIncrementalImport" and (Hive_DB == None or Hive_Table == None or displayHelp == True):
		print_resetIncrementalImport_help()

	if operation == "resetCDCImport" and (Hive_DB == None or Hive_Table == None or displayHelp == True):
		print_resetCDCImport_help()

	if operation == "clearExportStage" and (connectionAlias == None or jdbcSchema == None or jdbcTable == None or displayHelp == True):
		print_clearExportStage_help()

	if operation == "repairIncrementalExport" and (connectionAlias == None or jdbcSchema == None or jdbcTable == None or displayHelp == True):
		print_repairIncrementalExport_help()

	if operation == "resetIncrementalExport" and (connectionAlias == None or jdbcSchema == None or jdbcTable == None or displayHelp == True):
		print_resetIncrementalExport_help()

	if operation == "dropExportTable" and (connectionAlias == None or jdbcSchema == None or jdbcTable == None or displayHelp == True):
		print_dropExportTable_help()

	if operation == "addImportTable" and (Hive_DB == None or connectionAlias == None or displayHelp == True):
		print_addImportTable_help()

	if operation == "addExportTable" and (connectionAlias == None or jdbcSchema == None or displayHelp == True):
		print_addExportTable_help()

	if operation == "testConnection" and (connectionAlias == None or displayHelp == True):
		print_testConnection_help()

	if operation == "runJDBCQuery" and (connectionAlias == None or displayHelp == True):
		print_runJDBCQuery_help()

	if operation == "runHiveQuery" and (jdbcQuery == None or jdbcQuery == "--help" or displayHelp == True):
		print_runHiveScript_help()

	if operation == "runHiveScript" and (hiveScript == None or hiveScript == "--help" or displayHelp == True):
		print_runHiveScript_help()

	if operation == "airflowGenerate" and displayHelp == True:
		print_airflow_help()

	if airflowAutoDAGonly == True and (airflowDAG != None or displayHelp == True):
		print_airflow_help()

	if operation == "discoverAtlasRdbms" and (connectionAlias == None or displayHelp == True):
		print_discoverAtlasRdbms_help()

	if operation == "checkAirflowExecution" and displayHelp == True:
		print_checkAirflowExecution_help()

	if operation == "sendAirflowStopMessage" and (airflowDAG == None or displayHelp == True):
		print_sendAirflowStopMessage_help()

	if quietMode == False:
		printHeader()

	# Initiate the logging functions with the correct level
	if loggingLevel == logging.DEBUG:
		logging.basicConfig(stream=sys.stdout, format='%(levelname)s %(funcName)s - %(message)s', level=loggingLevel)
	else:
		logging.basicConfig(stream=sys.stdout, format='%(levelname)s - %(message)s', level=loggingLevel)


	# Actual operation is started here and is handled in one big Try block
	try:
		if operation == "runHiveScript":
			commonOperation = common_operations.operation()
			commonOperation.common_config.checkKerberosTicket()
			returnCode = commonOperation.executeBeelineScript(hiveScriptFile=hiveScript, useDB=Hive_DB)
			if returnCode != 0:
				raise SQLerror("Hive Query Error")
			commonOperation.common_config.remove_temporary_files()

		if operation == "runHiveQuery":
			commonOperation = common_operations.operation()
			commonOperation.common_config.checkKerberosTicket()
			commonOperation.connectToHive(forceSkipTest=True, quiet=quietMode)
			result = commonOperation.executeHiveQuery(query=jdbcQuery, quiet=quietMode)
			if result.empty == True:
				print("SQL generated no output")
			else:
				blankIndex=[''] * len(result)
				result.index=blankIndex
				print(result.to_string())
			commonOperation.common_config.remove_temporary_files()
	
		if operation == "discoverAtlasRdbms":
			commonConfig = common_config.config()
			atlasOperation = atlas_operations.atlasOperation()
			if commonConfig.checkConnectionAlias(connectionAlias) == False:
				logging.error("The specified database connection does not exist.")
				sys.exit(1)

			# Check if the schema is correct
			if atlasOperation.checkAtlasSchema() == False:
				sys.exit(1)

			configObject = commonConfig.getAtlasDiscoverConfigObject(connectionAlias)
			atlasOperation.setConfiguration(configObject)
			atlasOperation.discoverAtlasRdbms()

		if operation == "runJDBCQuery":
			commonConfig = common_config.config()
			if commonConfig.checkConnectionAlias(connectionAlias) == False:
				logging.error("The specified database connection does not exist.")
				sys.exit(1)
			commonConfig.lookupConnectionAlias(connectionAlias)
			commonConfig.connectToJDBC()
			result = commonConfig.executeJDBCquery(query=jdbcQuery)
			if result.empty == True:
				print("SQL generated no output")
			else:
				blankIndex=[''] * len(result)
				result.index=blankIndex
				print(result.to_string())
			commonConfig.remove_temporary_files()

		if operation == "clearImportStage":
			importConfig = import_config.config(Hive_DB, Hive_Table)
			importConfig.clearStage()
			logging.info("Stage information cleared for %s.%s"%(Hive_DB, Hive_Table))
			importConfig.remove_temporary_files()

		if operation == "checkAirflowExecution":
			commonOperation = common_operations.operation()
			commonOperation.common_config.checkKerberosTicket()
			airflow = Airflow.initialize()
			airflow.checkExecution()
			airflow.sendJSON(airflowDAG=airflowDAG, status="started")
			airflow.common_config.remove_temporary_files()

		if operation == "sendAirflowStopMessage":
			commonOperation = common_operations.operation()
			commonOperation.common_config.checkKerberosTicket()
			airflow = Airflow.initialize()
			airflow.sendJSON(airflowDAG=airflowDAG, status="finished")
			airflow.common_config.remove_temporary_files()

		if operation == "airflowGenerate":
			airflow = Airflow.initialize()
			if airflowAutoDAGonly == False and airflowDAG == None:
				print ("You havent specified --airflowAutoDAGonly or a name of a DAG. That means that you want to generate ALL Airflow DAG")
				print ("More information about valid options can be found if executed with --help")
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)

				if answer.lower() != "y":
					logging.info("No generation of Airflow DAG's will happen")
					sys.exit(1)

			airflow.generateDAG(name=airflowDAG, autoDAGonly=airflowAutoDAGonly, writeDAG=airflowWriteDAG, DAGFolder=airflowDAGFolder)
			airflow.common_config.remove_temporary_files()

		if operation == "encryptInstance":
			print("This function will encrypt a username and password for the specified DBImport Instance. Any previous username/password will be overwritten")
			print("There is no undo function for this, so if you want to cancel, please press Ctrl-C")

			try:
				DBImportInstance = input("DBImport Instance: ")
			except KeyboardInterrupt:
				print ("")
				logging.warning("Aborting command by user request")
				sys.exit(1)

			copyOperation = copy_operations.operation()
			if copyOperation.checkDBImportInstance(instance = DBImportInstance) == False:
				logging.error("The specified DBImport instance does not exist.")
				sys.exit(1)

			try:
				username = input("Username: ")
				password = input("Password: ")
			except KeyboardInterrupt:
				print ("")
				logging.warning("Aborting command by user request")
				sys.exit(1)

			copyOperation.encryptUserPassword(instance = DBImportInstance, username = username, password = password)
			copyOperation.remove_temporary_files()

		if operation == "encryptCredentials":
			print("This function will encrypt a username and password for the specified Database Connection. Any previous username/password will be overwritten")
			print("There is no undo function for this, so if you want to cancel, please press Ctrl-C")
			if connectionAlias == None:
				try:
					connectionAlias = input("DataBase Connection (dbalias): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)

			commonConfig = common_config.config()
			if commonConfig.checkConnectionAlias(connectionAlias) == False:
				logging.error("The specified database connection does not exist.")
				sys.exit(1)

			commonConfig.lookupConnectionAlias(connection_alias=connectionAlias, decryptCredentials=False)

			try:
				if commonConfig.db_awss3 == True:
					username = input("AccessKeyId: ")
					password = input("SecretAccessKey: ")
				else:
					username = input("Username: ")
					password = input("Password: ")
			except KeyboardInterrupt:
				print ("")
				logging.warning("Aborting command by user request")
				sys.exit(1)

			commonConfig.encryptUserPassword(connectionAlias, username, password)
			commonConfig.remove_temporary_files()

		if operation == "encryptString":
			print("This function will encrypt a string. That string can then be used as a username or password in the configuration file.")
			try:
				strToEncrypt = input("String: ")
			except KeyboardInterrupt:
				print ("")
				logging.warning("Aborting command by user request")
				sys.exit(1)

			commonConfig = common_config.config()
			print(strToEncrypt)
			encryptedStr = commonConfig.encryptString(strToEncrypt)
			print(encryptedStr)
			commonConfig.remove_temporary_files()

#		if operation == "getAllConnectionPasswords":
#			commonConfig = common_config.config()
#			commonConfig.printConnectionAliasDetails()

		if operation == "repairAllIncrementalImports":
			print("This command will search for incremental tables that have an entry in 'import_stage'. If the entry exists,")
			print("it will fetch the max value from the hive table and update the MySQL database. This value will then be used")
			print("used as the starting point for the next sqoop import.")
			print("")
			if autoYesAnswer == False:
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)
			else:
				answer = "y"

			if answer.lower() == "y":
				import_operation = import_operations.operation()
				import_operation.repairAllIncrementalImports()
				import_operation.remove_temporary_files()
			else:
				logging.info("Didnt answer 'y'. No actions performed")


		if operation == "repairIncrementalImport":
			import_operation = import_operations.operation(Hive_DB, Hive_Table)
			if import_operation.import_config.common_config.checkKerberosTicket() == False:
				logging.error("There is no valid Kerberos ticket available. Please create one before running this command")
				import_operation.remove_temporary_files()
				sys.exit(1)

			print("This command will try to repair an incremental import that have come out-of-sync between data in the source table")
			print("and the hive table. The tool will connect to the hive database and read the max value for the column")
			print("that is used for the incremental loads. This value will then be used as the starting point for the next sqoop import")
			print("")
			if autoYesAnswer == False:
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)
			else:
				answer = "y"

			if answer.lower() == "y":
				import_operation.resetIncrMaxValue()
				import_operation.clearStage()
				import_operation.remove_temporary_files()
			else:
				logging.info("Didnt answer 'y'. No actions performed")

		if operation == "resetIncrementalImport":
			import_operation = import_operations.operation(Hive_DB, Hive_Table)
			if import_operation.import_config.common_config.checkKerberosTicket() == False:
				logging.error("There is no valid Kerberos ticket available. Please create one before running this command")
				import_operation.remove_temporary_files()
				sys.exit(1)

			print("This command will truncate the Target Table and force the next import to be a full import.")
			print("")
			if autoYesAnswer == False:
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)
			else:
				answer = "y"

			if answer.lower() == "y":
				import_operation.resetIncrMinMaxValues(maxValue=None)
				import_operation.truncateTargetTable()
				import_operation.clearStage()
				import_operation.remove_temporary_files()
			else:
				logging.info("Didnt answer 'y'. No actions performed")

		if operation == "resetCDCImport":
			import_operation = import_operations.operation(Hive_DB, Hive_Table)
			if import_operation.import_config.common_config.checkKerberosTicket() == False:
				logging.error("There is no valid Kerberos ticket available. Please create one before running this command")
				import_operation.remove_temporary_files()
				sys.exit(1)

			print("This command will truncate the Target Table and force the next import to be a full import.")
			print("")
			if autoYesAnswer == False:
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)
			else:
				answer = "y"

			if answer.lower() == "y":
				import_operation.resetIncrMinMaxValues(maxValue=None)
				import_operation.truncateTargetTable()
				import_operation.clearStage()
				import_operation.remove_temporary_files()
			else:
				logging.info("Didnt answer 'y'. No actions performed")

		if operation == "resetIncrementalExport":
			print("This command will truncate the Target Table and force the next export to be a full export.")
			print("")
			if autoYesAnswer == False:
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)
			else:
				answer = "y"

			if answer.lower() == "y":
				export_operation = export_operations.operation(connectionAlias=connectionAlias, targetSchema=jdbcSchema,targetTable=jdbcTable)
				export_operation.truncateJDBCTable(force=True)
				export_operation.resetIncrMinMaxValues(maxValue=None)
				export_operation.clearStage()
				export_operation.remove_temporary_files()
			else:
				logging.info("Didnt answer 'y'. No actions performed")

		if operation == "repairIncrementalExport":
			print ("This command will repair an incremental import by reading the max value for the incremental column from the ")
			print ("Target Table and use that as the max value for the next export.")
			print("")
	
			if autoYesAnswer == False:
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)
			else:
				answer = "y"

			if answer.lower() == "y":
				export_operation = export_operations.operation(connectionAlias=connectionAlias, targetSchema=jdbcSchema,targetTable=jdbcTable)
				export_operation.resetMaxValueFromTarget()
				export_operation.remove_temporary_files()
			else:
				logging.info("Didnt answer 'y'. No actions performed")

		if operation == "dropExportTable":
			print("This command will drop the Target Table and if it's an incremental export, force the next export to be a full export.")
			print("")

			if autoYesAnswer == False:
				try:
					answer = input("Are you sure you want to do this? (y/N): ")
				except KeyboardInterrupt:
					print ("")
					logging.warning("Aborting command by user request")
					sys.exit(1)
			else:
				answer = "y"

			if answer.lower() == "y":
				export_operation = export_operations.operation(connectionAlias=connectionAlias, targetSchema=jdbcSchema,targetTable=jdbcTable)
				export_operation.dropJDBCTable()
				export_operation.resetIncrMinMaxValues(maxValue=None)
				export_operation.clearStage()
				export_operation.remove_temporary_files()
			else:
				logging.info("Didnt answer 'y'. No actions performed")

		if operation == "clearExportStage":
			export_operation = export_operations.operation(connectionAlias=connectionAlias, targetSchema=jdbcSchema,targetTable=jdbcTable)
			export_operation.clearStage()
			logging.info("Stage information cleared")

			export_operation.remove_temporary_files()

		if operation == "addImportTable":
			import_operation = import_operations.operation()
			import_operation.discoverAndAddTablesFromSource(
				dbalias=connectionAlias, 
				hiveDB=Hive_DB, 
				schemaFilter=jdbcSchema, 
				tableFilter=jdbcTable,
				addSchemaToTable=addSchemaToTable,
				addCustomText=addCustomText,
				addCounterToTable=addCounterToTable,
				counterStart=counterStart,
				onlyJsonOutput=jsonOutput)
		
		if operation == "addExportTable":
			export_operation = export_operations.operation()
			export_operation.discoverAndAddTablesFromHive(
				dbFilter=Hive_DB, 
				tableFilter=Hive_Table,
				dbalias=connectionAlias, 
				schema=jdbcSchema, 
				addDBToTable=addDBToTable,
				addCustomText=addCustomText,
				addCounterToTable=addCounterToTable,
				counterStart=counterStart)
		
		if operation == "testConnection":
			commonConfig = common_config.config()
			if commonConfig.checkConnectionAlias(connectionAlias) == False:
				logging.error("The specified database connection does not exist.")
				sys.exit(1)
			commonConfig.lookupConnectionAlias(connectionAlias)
			if commonConfig.db_awss3 == True:
				logging.error("AWS S3 is not supported for '--testConnection' operations")
			else:
				commonConfig.connectToJDBC()
				print("Connected successfully")

			commonConfig.disconnectFromJDBC()
#			commonConfig.connectToImpala()
#			commonConfig.disconnectFromImpala()

			commonConfig.remove_temporary_files()

	# Error handling for the entire stage block
	except invalidConfiguration as errMsg:
		if str(errMsg) != "":
			logging.error(errMsg)
		if commonConfig != None:
			commonConfig.remove_temporary_files()
		if importConfig != None:
			importConfig.remove_temporary_files()
		elif import_operation != None:
			import_operation.import_config.remove_temporary_files()
		elif export_operation != None:
			export_operation.export_config.remove_temporary_files()
		elif copy_operation != None:
			copy_operation.import_config.remove_temporary_files()
		sys.exit(1)
	except RuntimeError as errMsg:
		import_operation.import_config.remove_temporary_files()
		sys.exit(1)
	except daemonExit as errMsg:
		logging.error("Program exit because requested to do so by developer. %s"%(errMsg))
		import_operation.import_config.remove_temporary_files()
		sys.exit(1)
	except SQLerror as errMsg:
		logging.error("SQL Error detected")
		logging.error(errMsg)
		if commonConfig != None:
			commonConfig.remove_temporary_files()
		if importConfig != None:
			importConfig.remove_temporary_files()
		elif import_operation != None:
			import_operation.import_config.remove_temporary_files()
		elif export_operation != None:
			export_operation.export_config.remove_temporary_files()
		elif copy_operation != None:
			copy_operation.import_config.remove_temporary_files()
		sys.exit(1)
	except:
		if commonConfig != None:
			commonConfig.remove_temporary_files()
		if importConfig != None:
			importConfig.remove_temporary_files()
		elif import_operation != None:
			import_operation.import_config.remove_temporary_files()
		elif export_operation != None:
			export_operation.export_config.remove_temporary_files()
		elif copy_operation != None:
			copy_operation.import_config.remove_temporary_files()
		raise
		sys.exit(1)

	sys.exit(0)


if __name__ == "__main__":
	main(sys.argv[1:])
